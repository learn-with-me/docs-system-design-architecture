{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Interview Preparation I'll keep documenting topics based on my understanding or from different sources I come across, where I couldn't have done better job in explaining the topic. One of the goals of this project is to serve as a guide for refreshing my memory on designing & architecture complex systems. Sure, there are books out there, but none goes in the depth I want myself to go into, and it is fun writing your on notes. Why not? What's the point? There is none, if you see none. I think these interviews are great. These challenge you into thinking how to build a system, if you were tasked to do so. Learning the fundamentals will set you up for success in your role, not only empower you to write better design documents, but also friendly challenge someone else's design as well to help bullet-proof their solution. How to prepare? Make sure you understand the fundamentals Stay fluent in solving some of the industry standard problems Think about how these concepts apply to your current/past job and where you used them or could have used them.","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#interview-preparation","text":"I'll keep documenting topics based on my understanding or from different sources I come across, where I couldn't have done better job in explaining the topic. One of the goals of this project is to serve as a guide for refreshing my memory on designing & architecture complex systems. Sure, there are books out there, but none goes in the depth I want myself to go into, and it is fun writing your on notes. Why not?","title":"Interview Preparation"},{"location":"#whats-the-point","text":"There is none, if you see none. I think these interviews are great. These challenge you into thinking how to build a system, if you were tasked to do so. Learning the fundamentals will set you up for success in your role, not only empower you to write better design documents, but also friendly challenge someone else's design as well to help bullet-proof their solution.","title":"What's the point?"},{"location":"#how-to-prepare","text":"Make sure you understand the fundamentals Stay fluent in solving some of the industry standard problems Think about how these concepts apply to your current/past job and where you used them or could have used them.","title":"How to prepare?"},{"location":"03-system-design-fundamentals/","text":"System Design Topics There is no end to what you can know about designing systems. However when preparing for interviews, make sure you are aware of most commonly asked questions/concepts Concepts Fundamental Concepts Networks & Protocols (IP, DNS, HTTP, TCP etc) Storage, Latency & Throughput Availability Caching Proxies Load Balancing Consistent Hashing Databases Leader Election Polling, Streaming, Sockets Endpoint Protection Messages & Pub-Sub Advanced Concepts Database Internals Papers Published Amazon - Dynamo DB pdf Facebook - Cassandra pdf Facebook - Scaling Memcache at Facebook pdf Google - Web Search pdf LinkedIn - Expresso Document DB pdf System to Design Fundamental Design a Rate Limiter Design Consistent Hashing Design a key-value store Design a Unique ID Generator in distributed system Url Shortener Intermediate Design a Typeahead (search auto-complete) Advanced Design a Web Crawler Design a Notification System Design a News Feed System Design a Chat System Design YouTube/Netflix Design Google Drive References FreeCodeCamp - System Design Fundamentals GitHub - Donne Martin - System Design Primer","title":"System Design Topics"},{"location":"03-system-design-fundamentals/#system-design-topics","text":"There is no end to what you can know about designing systems. However when preparing for interviews, make sure you are aware of most commonly asked questions/concepts","title":"System Design Topics"},{"location":"03-system-design-fundamentals/#concepts","text":"","title":"Concepts"},{"location":"03-system-design-fundamentals/#fundamental-concepts","text":"Networks & Protocols (IP, DNS, HTTP, TCP etc) Storage, Latency & Throughput Availability Caching Proxies Load Balancing Consistent Hashing Databases Leader Election Polling, Streaming, Sockets Endpoint Protection Messages & Pub-Sub","title":"Fundamental Concepts"},{"location":"03-system-design-fundamentals/#advanced-concepts","text":"Database Internals","title":"Advanced Concepts"},{"location":"03-system-design-fundamentals/#papers-published","text":"Amazon - Dynamo DB pdf Facebook - Cassandra pdf Facebook - Scaling Memcache at Facebook pdf Google - Web Search pdf LinkedIn - Expresso Document DB pdf","title":"Papers Published"},{"location":"03-system-design-fundamentals/#system-to-design","text":"","title":"System to Design"},{"location":"03-system-design-fundamentals/#fundamental","text":"Design a Rate Limiter Design Consistent Hashing Design a key-value store Design a Unique ID Generator in distributed system Url Shortener","title":"Fundamental"},{"location":"03-system-design-fundamentals/#intermediate","text":"Design a Typeahead (search auto-complete)","title":"Intermediate"},{"location":"03-system-design-fundamentals/#advanced","text":"Design a Web Crawler Design a Notification System Design a News Feed System Design a Chat System Design YouTube/Netflix Design Google Drive","title":"Advanced"},{"location":"03-system-design-fundamentals/#references","text":"FreeCodeCamp - System Design Fundamentals GitHub - Donne Martin - System Design Primer","title":"References"},{"location":"getting-started/","text":"Getting Started Organizing yourself is the first step. Here is my list of resources that I would follow Data Structure & Algorithms Books Cracking the Coding Interview, 6th Edition , Gayle Laakmann McDowell Elements of Programming Interviews in Java , Adnan Aziz Practice Problems LeetCode System Design Books System Design Interview, 2nd Edition , Alex Xu Designing Data-Intensive Applications , Martin Kleppmann Courses Grokking the System Design Interview Published Papers DynamoDB etc","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"Organizing yourself is the first step. Here is my list of resources that I would follow","title":"Getting Started"},{"location":"getting-started/#data-structure-algorithms","text":"","title":"Data Structure &amp; Algorithms"},{"location":"getting-started/#books","text":"Cracking the Coding Interview, 6th Edition , Gayle Laakmann McDowell Elements of Programming Interviews in Java , Adnan Aziz","title":"Books"},{"location":"getting-started/#practice-problems","text":"LeetCode","title":"Practice Problems"},{"location":"getting-started/#system-design","text":"","title":"System Design"},{"location":"getting-started/#books_1","text":"System Design Interview, 2nd Edition , Alex Xu Designing Data-Intensive Applications , Martin Kleppmann","title":"Books"},{"location":"getting-started/#courses","text":"Grokking the System Design Interview","title":"Courses"},{"location":"getting-started/#published-papers","text":"DynamoDB etc","title":"Published Papers"},{"location":"glossary/","text":"Glossary Capacity Answers question about how much do I have to host in a system in a given time? This question can be answered for persistent storage as well as in-memory storage. Data Integrity Answers questions like: is all the data being written to storage? is all the data being read from storage? is all the data safe in the storage? Fault Tolerance Answers question - can I handle something when failures take place Observability A term used to monitor health and performance of a system. It tells you when the metrics represents a problem or is about to be one. Observability alerts you on such events, makes you aware of the business impacts, the indicator(s) of impact, and also guides you towards how to fix the problem. Throughput and Latency Throughput - explains how many requests can be processed and completed, by a system in any given amount of time (usually measured per second). Requests can be in terms of API request, Database queries, or even messages in a queue. Latency - explains how fast those requests can be processed in a given amount of time (usually measured per second). References Latency vs Throughput YT - Latency vs Throughput | System Design Essentials YT - Latency vs Throughput What is the Difference Between Bandwidth and Latency? Latency vs Throughput \u2013 Understanding the Difference Network Latency vs. Throughput vs. Bandwidth Bandwidth vs. Latency: What is the Difference? Throughput Vs. Latency \u2013 What\u2019s the Difference? What Is Network Latency? Understanding Network Bandwidth vs Latency","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#capacity","text":"Answers question about how much do I have to host in a system in a given time? This question can be answered for persistent storage as well as in-memory storage.","title":"Capacity"},{"location":"glossary/#data-integrity","text":"Answers questions like: is all the data being written to storage? is all the data being read from storage? is all the data safe in the storage?","title":"Data Integrity"},{"location":"glossary/#fault-tolerance","text":"Answers question - can I handle something when failures take place","title":"Fault Tolerance"},{"location":"glossary/#observability","text":"A term used to monitor health and performance of a system. It tells you when the metrics represents a problem or is about to be one. Observability alerts you on such events, makes you aware of the business impacts, the indicator(s) of impact, and also guides you towards how to fix the problem.","title":"Observability"},{"location":"glossary/#throughput-and-latency","text":"Throughput - explains how many requests can be processed and completed, by a system in any given amount of time (usually measured per second). Requests can be in terms of API request, Database queries, or even messages in a queue. Latency - explains how fast those requests can be processed in a given amount of time (usually measured per second).","title":"Throughput and Latency"},{"location":"glossary/#references","text":"Latency vs Throughput YT - Latency vs Throughput | System Design Essentials YT - Latency vs Throughput What is the Difference Between Bandwidth and Latency? Latency vs Throughput \u2013 Understanding the Difference Network Latency vs. Throughput vs. Bandwidth Bandwidth vs. Latency: What is the Difference? Throughput Vs. Latency \u2013 What\u2019s the Difference? What Is Network Latency? Understanding Network Bandwidth vs Latency","title":"References"},{"location":"Sort/","text":"To be sorted Calculate network bandwidth requirement How to calculate network bandwidth requirements for a video sharing site like YouTube. Assume average video size is 100MB and 500000 users uploading 1 video per day. 5 million daily active users seeing 5 videos per day. You will probably not upload all video to 1 DC and 1 host. How many DC and hosts will you use? Let\u2019s assume we need this globally. So 4 availability zones, 2 DC each, 2 hosts per DC Design YouTube view history Design YouTube view history. Discuss system architecture, data storage etc. What is the storage system you would use, why and how? Sync data for large number of devices Design of how data is synced when there are large number of devices and data is updating rapidly. Like we wear fitness bands, data is updated with every step in the band and then synced with the global storage. What is the point of sync that on every step? There are multiple reasons of not to do that it's expensive the device doesn't have an internet connection all the time it consumes a lot of battery and probably the main reason: it is not needed to have such an latency on syncing that. it's not a HFT thing Then how is data synced behind the scenes? What is the architecture? I'd upload the data (whatever precision is required) into a cell phone via bt and just send an butch update to the server. DB depends on requirements, might be wide column, or time series db Consistent Hashing https://www.acodersjourney.com/system-design-interview-consistent-hashing/ Fanout Implementation What's your thought for fanout implementation - listen change stream + lambda instead of a queue + consumer? consider you have high write throughput. I wanted to understand the limitation of Lambda. I got the answer. Example, timeline update for follower in twitter system. If you want to build listen change stream you need to use a queue internally + consumer. Lambda I dont recommend lambda for low latency, HA and high throughput usecases. Lambda cold starts are pretty well knows time consumers and you'd have to account to the latency addition (sometimes in seconds). https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/ You won't have cold start with Provisioned Concurrency. You are gonna pay for it and it's not cheap. basically what PC does is to warm up some additional containers (more containers, more $), ready to serve a request. also say you have 100 containers ready and you get a burst call of 101 requests which makes the last call a cold-start one. Other References https://databricks.com/glossary/lambda-architecture https://en.wikipedia.org/wiki/Lambda_architecture Design Instagram vs Twitter Can I say that system design for twitter vs instagram is the same except that twitter has 140 char limit? Rest seems to be same. user posts txt, img & video. Fanout to followers with celebrity as special case. Read heavy.. say 10 times of write.. eventual consistency is fine. follow part is totaly different in twitter and insta Instagram should be media heavy than Twitter The way news feed is computed is totally different in both. The main idea of any news feed is fanout. In write heavy scenario. if we have more reads that writes its better to spend more time in write to prerate data so that read will be cheaper. Kafka It is worth noting that typically only one consumer within a consumer group is subscribed to a partition... [1] this is how Kafka achieves high message processing throughput. So, even in the case of multiple partitions, messages within a single partition are truly processed in the order they were sent. Also worth noting is that partitions are specific to Kafka. For example, Google Cloud Pub/Sub doesn\u2019t expose partitions to the user \u2014 they are there, but they are behind the scenes. [2]. In a systems design interview, it is sufficient to just talk about topics and subscribers. There is no need to go deeper than topics or to mention specific products. In fact, I am told by an insider that mentioning specific products is frowned upon at Google, which has its own version of all the open-source projects (and more). [1] \u201cEach partition is connected to at most one consumer from a group.\u201d https://blog.cloudera.com/scalability-of-kafka-messaging-using-consumer-groups/ [2] \u201cPartitions are not exposed to users.\u201d https://medium.com/google-cloud/google-cloud-pub-sub-ordered-delivery-1e4181f60bc8 Confluent Build Services on a Backbone of Events Confluent Apache Kafka Supports 200K Partitions Per Cluster Kafka vs Redis Pub-Sub, Differences which you should know YT - Kafka: A Modern Distributed System Distributed Consensus To get Redis to be fault tolerant for this use case, you need to give up the idea of a partially synchronous system, or you need to sync up state using transactions, or you need a Redis master node. Basically to have consistent state in Redis is difficult and to get it, you need to give up a lot. So one answer is a distributed consensus algo, where at least the majority of nodes agree on state. This eliminates a lot of the problems and possible bottlenecks that Redis might have. But Replicated State machines, configuration stores, leader election, distributed locking are all very good use cases for distribute consensus. Distributed concensus algos, in general, allow you to cheat some of the problems that \"normal\" concensus patterns might have. There's no free lunch of course, these algos have serious problems with round trip times. and other concerns. It's not some magic solution. If that data can't change, it reduces the complexity and need for these algos. https://sre.google/sre-book/managing-critical-state/ Open Questions LLD design for Log4J library Really Random things https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/ https://blog.dream11engineering.com/building-scalable-real-time-analytics-alerting-and-anomaly-detection-architecture-at-dream11-e20edec91d33 - scaling for dimensionality, aggregated data, metrics after I collect the data. Collecting the data from devices. around 1.2 billions devices a day. Edge computing means at the device, some level of aggregations should happen and collect the data through HTTP through Kafka by topic https://medium.com/@jadsarmo/why-we-chose-java-for-our-high-frequency-trading-application-600f7c04da94 https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96 Cloudflare Cloudflare architecture and how BPF eats the world - incredible articles Cloudflare CDN Caching - Dynamic content is generated by scripts that change the content on a page. By running scripts in a CDN cache instead of in a distant origin server, dynamic content can be generated and delivered from a cache. Dynamic content is thus essentially \"cached\" and does not have to be served all the way from the origin, reducing the response time to client requests and speeding up dynamic webpages. Cloudflare Workers, for example, are serverless JavaScript functions that run on the Cloudflare CDN A Closer Look At Etcd: The Brain Of A Kubernetes Cluster etcd will almost always be the bottleneck because everything is stored there by API Server AirBnb - Avoiding Double Payments in a Distributed Payments System https://logz.io/blog/kafka-vs-redis/ https://www.educba.com/redis-vs-kafka/ YT Query Petabyte Scale Dataset on S3 YT https://www.youtube.com/channel/UCZEfiXy7PmtVTezYUvc4zZw - 14 videos on this YouTube channel. This dude has nice videos, beginner friendly. The databases video summed it up pretty well, i would suggest it to beginners aswell who want a databases overview YT MIT 6.824 Distributed Systems (Spring 2020) YT Four Distributed Systems Architectural Patterns by Tim Berglund YT Designing Udemy's Taxonomy on SQL | System Design YT Cancel token YT - Airbnb Search Architecture YT - Consistent Hashing Rajeev - Basically, nodes are arranged in a ring structure (not actually a ring but conceptually). Each node is responsible for keys hashed between it and its predecessor in the ring. Node placement has nothing to do with hash function used or how keys are hashed- instead it\u2019s based on data, mostly to spread data evenly across all nodes. A virtual node placed in the ring structure would relieve load on its successor. YT - why it is very hard to cancel an HTTP request YT - AWS re:Invent 2017: Architecting a data lake with Amazon S3, Amazon Kinesis, and Ama (ABD318) - Atlassian built a data lake from S3, Kinesis, Glue, and Athena YT - Episode 109: eBay\u2019s Architecture Principles with Randy Shoup - a great podcast by Randy Shoup ( VP Engineering and Chief Architect at Ebay) on ebay Architecture principles YT - Streaming a Million Likes/Second: Real-Time Interactions on Live Video YT - Watermarks: Time and Progress in Apache Beam and Beyond - about event time ordering https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design - references for API design best practices. Very comprehensive collection of best practices https://docs.microsoft.com/en-us/azure/architecture/patterns/index-patterns Java Prep https://docs.oracle.com/javase/tutorial/ https://github.com/javadroider/interview-prep/tree/master/interview https://netflix.github.io/atlas-docs/overview/ https://leetcode.com/problems/flatten-a-multilevel-doubly-linked-list/ https://leetcode.com/problems/median-of-two-sorted-arrays/ - pretty much needs a big trick/2 and even then the edge cases are hard to explain/code in the timeframe, esp without prev practice. U can share ur screen with sketchboard.io or digital pad+pencil works well with google jamboard. I simply share my screen on hangouts or zoom and diacuss on sketchboard.io If thats not possible then macbook sidecar with ipad and using this i can write or draw on any digital whiteboard. I have tried with jamboard. DDIA Notes DDIA notes - keyvanakbary DDIA notes - ibillett DDIA notes - xx DDIA Chapter 1 https://chiragshah9696.medium.com/interviews-interviews-interviews-7407faf4c7cc https://betterprogramming.pub/top-30-apple-coding-interview-questions-with-solutions-19990071ebfc MongoDB Reddit - cons of mongodb HSBC moves to MongoDB end to end system design with data generation, data ingestion, data aggregation, and data representation XMPP is decentralized whereas WebSockets are centralized. XMPP works on the application layer whereas WebSockets are a transport protocol https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/ Leaderboard - Building real-time Leaderboard with Redis What Is Load Balancing? Types, Configurations, and Best Tools - DNSstuff Architecture blogs Fraud Detection - at the simplest, it's just a stream of events and some query or model looking over the data AWS Architecture Overview - Fraud Detection Using Machine Learning Real-time fraud detection - Azure Example Scenarios Developing a flash sale system | HackerNoon News Feed Ranking Meta How machine learning powers Facebook\u2019s News Feed ranking algorithm How the Facebook Algorithm Works in 2023 Stack Overflow How did WhatsApp achieve 2 million connections per server? Stack Overflow Elasticsearch index sharding explanation https://microservices.io/ http://distributedsystemscourse.com/ Push notifications https://developer.apple.com/documentation/usernotifications/setting_up_a_remote_notification_server/pushing_background_updates_to_your_app Concurrency & Parallelsim Overview of Modern Concurrency and Parallelism Concepts Educative Multithreading notes Introduction to Thread Pools in Java | Baeldung Java Concurrency and Multithreading Tutorial REST vs RPC gRPC vs REST: Understanding gRPC, OpenAPI and REST and when to use them in API design | Google Cloud Blog YT Distributed Systems lecture series Coding / OO problems Design Tic Tac Toe - establish a contract of a board that an implementer would fulfill Snake and Ladder HyperLogLog - probabilistic data structure https://leetcode.com/problems/design-search-autocomplete-system/ https://www.oodesign.com https://github.com/iluwatar/java-design-patterns Bloom Filter Cassandra Bloom Filters Ribbon Filter https://serverfault.com/questions/238417/are-networks-now-faster-than-disks https://thenewstack.io/datadog-monitors-scalable-systems/ Write a time-series database engine from scratch Datadog How to solve 5 Elasticsearch performance and scaling problems Optimizing Flipkart\u2019s Serviceability Data from 300 GB to 150 MB in-memory Leadership Principles https://www.levels.fyi/blog/amazon-leadership-principles.html YT - Amazon Interrview Questions playlist https://druid.apache.org/ https://linkerd.io/ theoretical-maximum-number-of-open-tcp-connections-that-a-modern-linux Exclusive: a behind-the-scenes look at Facebook release engineering - explains a bit on how to copy 38 GB file located in US to all the 1000 Data Center located in China. All Data Center are loaded with work and using all bandwidth might affect the work. (BitTorrent like question) Sky Computing, the Next Era After Cloud Computing Sequence Generation in Cloud Spanner Stack Overflow Distributed Sequence Number Generation Redis INCR","title":"To be sorted"},{"location":"Sort/#to-be-sorted","text":"","title":"To be sorted"},{"location":"Sort/#calculate-network-bandwidth-requirement","text":"How to calculate network bandwidth requirements for a video sharing site like YouTube. Assume average video size is 100MB and 500000 users uploading 1 video per day. 5 million daily active users seeing 5 videos per day. You will probably not upload all video to 1 DC and 1 host. How many DC and hosts will you use? Let\u2019s assume we need this globally. So 4 availability zones, 2 DC each, 2 hosts per DC","title":"Calculate network bandwidth requirement"},{"location":"Sort/#design-youtube-view-history","text":"Design YouTube view history. Discuss system architecture, data storage etc. What is the storage system you would use, why and how?","title":"Design YouTube view history"},{"location":"Sort/#sync-data-for-large-number-of-devices","text":"Design of how data is synced when there are large number of devices and data is updating rapidly. Like we wear fitness bands, data is updated with every step in the band and then synced with the global storage. What is the point of sync that on every step? There are multiple reasons of not to do that it's expensive the device doesn't have an internet connection all the time it consumes a lot of battery and probably the main reason: it is not needed to have such an latency on syncing that. it's not a HFT thing Then how is data synced behind the scenes? What is the architecture? I'd upload the data (whatever precision is required) into a cell phone via bt and just send an butch update to the server. DB depends on requirements, might be wide column, or time series db","title":"Sync data for large number of devices"},{"location":"Sort/#consistent-hashing","text":"https://www.acodersjourney.com/system-design-interview-consistent-hashing/","title":"Consistent Hashing"},{"location":"Sort/#fanout-implementation","text":"What's your thought for fanout implementation - listen change stream + lambda instead of a queue + consumer? consider you have high write throughput. I wanted to understand the limitation of Lambda. I got the answer. Example, timeline update for follower in twitter system. If you want to build listen change stream you need to use a queue internally + consumer.","title":"Fanout Implementation"},{"location":"Sort/#lambda","text":"I dont recommend lambda for low latency, HA and high throughput usecases. Lambda cold starts are pretty well knows time consumers and you'd have to account to the latency addition (sometimes in seconds). https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/ You won't have cold start with Provisioned Concurrency. You are gonna pay for it and it's not cheap. basically what PC does is to warm up some additional containers (more containers, more $), ready to serve a request. also say you have 100 containers ready and you get a burst call of 101 requests which makes the last call a cold-start one.","title":"Lambda"},{"location":"Sort/#other-references","text":"https://databricks.com/glossary/lambda-architecture https://en.wikipedia.org/wiki/Lambda_architecture","title":"Other References"},{"location":"Sort/#design-instagram-vs-twitter","text":"Can I say that system design for twitter vs instagram is the same except that twitter has 140 char limit? Rest seems to be same. user posts txt, img & video. Fanout to followers with celebrity as special case. Read heavy.. say 10 times of write.. eventual consistency is fine. follow part is totaly different in twitter and insta Instagram should be media heavy than Twitter The way news feed is computed is totally different in both. The main idea of any news feed is fanout. In write heavy scenario. if we have more reads that writes its better to spend more time in write to prerate data so that read will be cheaper.","title":"Design Instagram vs Twitter"},{"location":"Sort/#kafka","text":"It is worth noting that typically only one consumer within a consumer group is subscribed to a partition... [1] this is how Kafka achieves high message processing throughput. So, even in the case of multiple partitions, messages within a single partition are truly processed in the order they were sent. Also worth noting is that partitions are specific to Kafka. For example, Google Cloud Pub/Sub doesn\u2019t expose partitions to the user \u2014 they are there, but they are behind the scenes. [2]. In a systems design interview, it is sufficient to just talk about topics and subscribers. There is no need to go deeper than topics or to mention specific products. In fact, I am told by an insider that mentioning specific products is frowned upon at Google, which has its own version of all the open-source projects (and more). [1] \u201cEach partition is connected to at most one consumer from a group.\u201d https://blog.cloudera.com/scalability-of-kafka-messaging-using-consumer-groups/ [2] \u201cPartitions are not exposed to users.\u201d https://medium.com/google-cloud/google-cloud-pub-sub-ordered-delivery-1e4181f60bc8 Confluent Build Services on a Backbone of Events Confluent Apache Kafka Supports 200K Partitions Per Cluster Kafka vs Redis Pub-Sub, Differences which you should know YT - Kafka: A Modern Distributed System","title":"Kafka"},{"location":"Sort/#distributed-consensus","text":"To get Redis to be fault tolerant for this use case, you need to give up the idea of a partially synchronous system, or you need to sync up state using transactions, or you need a Redis master node. Basically to have consistent state in Redis is difficult and to get it, you need to give up a lot. So one answer is a distributed consensus algo, where at least the majority of nodes agree on state. This eliminates a lot of the problems and possible bottlenecks that Redis might have. But Replicated State machines, configuration stores, leader election, distributed locking are all very good use cases for distribute consensus. Distributed concensus algos, in general, allow you to cheat some of the problems that \"normal\" concensus patterns might have. There's no free lunch of course, these algos have serious problems with round trip times. and other concerns. It's not some magic solution. If that data can't change, it reduces the complexity and need for these algos. https://sre.google/sre-book/managing-critical-state/","title":"Distributed Consensus"},{"location":"Sort/#open-questions","text":"LLD design for Log4J library","title":"Open Questions"},{"location":"Sort/#really-random-things","text":"https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/ https://blog.dream11engineering.com/building-scalable-real-time-analytics-alerting-and-anomaly-detection-architecture-at-dream11-e20edec91d33 - scaling for dimensionality, aggregated data, metrics after I collect the data. Collecting the data from devices. around 1.2 billions devices a day. Edge computing means at the device, some level of aggregations should happen and collect the data through HTTP through Kafka by topic https://medium.com/@jadsarmo/why-we-chose-java-for-our-high-frequency-trading-application-600f7c04da94 https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96 Cloudflare Cloudflare architecture and how BPF eats the world - incredible articles Cloudflare CDN Caching - Dynamic content is generated by scripts that change the content on a page. By running scripts in a CDN cache instead of in a distant origin server, dynamic content can be generated and delivered from a cache. Dynamic content is thus essentially \"cached\" and does not have to be served all the way from the origin, reducing the response time to client requests and speeding up dynamic webpages. Cloudflare Workers, for example, are serverless JavaScript functions that run on the Cloudflare CDN A Closer Look At Etcd: The Brain Of A Kubernetes Cluster etcd will almost always be the bottleneck because everything is stored there by API Server AirBnb - Avoiding Double Payments in a Distributed Payments System https://logz.io/blog/kafka-vs-redis/ https://www.educba.com/redis-vs-kafka/ YT Query Petabyte Scale Dataset on S3 YT https://www.youtube.com/channel/UCZEfiXy7PmtVTezYUvc4zZw - 14 videos on this YouTube channel. This dude has nice videos, beginner friendly. The databases video summed it up pretty well, i would suggest it to beginners aswell who want a databases overview YT MIT 6.824 Distributed Systems (Spring 2020) YT Four Distributed Systems Architectural Patterns by Tim Berglund YT Designing Udemy's Taxonomy on SQL | System Design YT Cancel token YT - Airbnb Search Architecture YT - Consistent Hashing Rajeev - Basically, nodes are arranged in a ring structure (not actually a ring but conceptually). Each node is responsible for keys hashed between it and its predecessor in the ring. Node placement has nothing to do with hash function used or how keys are hashed- instead it\u2019s based on data, mostly to spread data evenly across all nodes. A virtual node placed in the ring structure would relieve load on its successor. YT - why it is very hard to cancel an HTTP request YT - AWS re:Invent 2017: Architecting a data lake with Amazon S3, Amazon Kinesis, and Ama (ABD318) - Atlassian built a data lake from S3, Kinesis, Glue, and Athena YT - Episode 109: eBay\u2019s Architecture Principles with Randy Shoup - a great podcast by Randy Shoup ( VP Engineering and Chief Architect at Ebay) on ebay Architecture principles YT - Streaming a Million Likes/Second: Real-Time Interactions on Live Video YT - Watermarks: Time and Progress in Apache Beam and Beyond - about event time ordering https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design - references for API design best practices. Very comprehensive collection of best practices https://docs.microsoft.com/en-us/azure/architecture/patterns/index-patterns Java Prep https://docs.oracle.com/javase/tutorial/ https://github.com/javadroider/interview-prep/tree/master/interview https://netflix.github.io/atlas-docs/overview/ https://leetcode.com/problems/flatten-a-multilevel-doubly-linked-list/ https://leetcode.com/problems/median-of-two-sorted-arrays/ - pretty much needs a big trick/2 and even then the edge cases are hard to explain/code in the timeframe, esp without prev practice. U can share ur screen with sketchboard.io or digital pad+pencil works well with google jamboard. I simply share my screen on hangouts or zoom and diacuss on sketchboard.io If thats not possible then macbook sidecar with ipad and using this i can write or draw on any digital whiteboard. I have tried with jamboard. DDIA Notes DDIA notes - keyvanakbary DDIA notes - ibillett DDIA notes - xx DDIA Chapter 1 https://chiragshah9696.medium.com/interviews-interviews-interviews-7407faf4c7cc https://betterprogramming.pub/top-30-apple-coding-interview-questions-with-solutions-19990071ebfc MongoDB Reddit - cons of mongodb HSBC moves to MongoDB end to end system design with data generation, data ingestion, data aggregation, and data representation XMPP is decentralized whereas WebSockets are centralized. XMPP works on the application layer whereas WebSockets are a transport protocol https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/ Leaderboard - Building real-time Leaderboard with Redis What Is Load Balancing? Types, Configurations, and Best Tools - DNSstuff Architecture blogs Fraud Detection - at the simplest, it's just a stream of events and some query or model looking over the data AWS Architecture Overview - Fraud Detection Using Machine Learning Real-time fraud detection - Azure Example Scenarios Developing a flash sale system | HackerNoon News Feed Ranking Meta How machine learning powers Facebook\u2019s News Feed ranking algorithm How the Facebook Algorithm Works in 2023 Stack Overflow How did WhatsApp achieve 2 million connections per server? Stack Overflow Elasticsearch index sharding explanation https://microservices.io/ http://distributedsystemscourse.com/ Push notifications https://developer.apple.com/documentation/usernotifications/setting_up_a_remote_notification_server/pushing_background_updates_to_your_app Concurrency & Parallelsim Overview of Modern Concurrency and Parallelism Concepts Educative Multithreading notes Introduction to Thread Pools in Java | Baeldung Java Concurrency and Multithreading Tutorial REST vs RPC gRPC vs REST: Understanding gRPC, OpenAPI and REST and when to use them in API design | Google Cloud Blog YT Distributed Systems lecture series Coding / OO problems Design Tic Tac Toe - establish a contract of a board that an implementer would fulfill Snake and Ladder HyperLogLog - probabilistic data structure https://leetcode.com/problems/design-search-autocomplete-system/ https://www.oodesign.com https://github.com/iluwatar/java-design-patterns Bloom Filter Cassandra Bloom Filters Ribbon Filter https://serverfault.com/questions/238417/are-networks-now-faster-than-disks https://thenewstack.io/datadog-monitors-scalable-systems/ Write a time-series database engine from scratch Datadog How to solve 5 Elasticsearch performance and scaling problems Optimizing Flipkart\u2019s Serviceability Data from 300 GB to 150 MB in-memory Leadership Principles https://www.levels.fyi/blog/amazon-leadership-principles.html YT - Amazon Interrview Questions playlist https://druid.apache.org/ https://linkerd.io/ theoretical-maximum-number-of-open-tcp-connections-that-a-modern-linux Exclusive: a behind-the-scenes look at Facebook release engineering - explains a bit on how to copy 38 GB file located in US to all the 1000 Data Center located in China. All Data Center are loaded with work and using all bandwidth might affect the work. (BitTorrent like question) Sky Computing, the Next Era After Cloud Computing Sequence Generation in Cloud Spanner Stack Overflow Distributed Sequence Number Generation Redis INCR","title":"Really Random things"},{"location":"Sort/availability/","text":"Availability Fail-over Active-Active seems similar to Master-Master replication. Are these both same or some difference between them? The two concepts are slightly different: active-active refers to a high availability configuration: several nodes offers the full service. In a normal situation, load-balancing occurs, to distribute the requests equally between all the active nodes. When one server is down, the requests are rerouted to the others. When used in a database context, there is only one database for the outside world. It is not specified how this is achieved: database servers can for example share the same data on (high availability) disks if the internal data structures are designed for concurrency. The advantage of active-active is to avoid having a very expensive spare-servers staying idle most of the cases (active-passive configuration). master-master replication (also called multi-master replication) refers to a specific database technique to synchronise database objects across several database instances in a way to ensure global consistency. The advantage is the flexibility, each participating database instance being well encapsulated, and hybrid replication scenarios are possible with several sets of masters. Conclusion: there is a tiny overlap between the two concepts: master-master could be used as one specific way to implement active-active for databases. But active-active can be implemented differently, and multi-master can have other purposes than high availability. AFAIK fail over active active is a kind of a HA (high availability deployment) of a server, where both servers are active (serving requests), via a load balancer (one goes down, the other gets more load) master master replication is a way for a distributed database cluster to have >1 source of truth (multiple master), by keeping the writes in sync using some consensus algo so, the 2 are not entirely the same","title":"Availability"},{"location":"Sort/availability/#availability","text":"Fail-over Active-Active seems similar to Master-Master replication. Are these both same or some difference between them? The two concepts are slightly different: active-active refers to a high availability configuration: several nodes offers the full service. In a normal situation, load-balancing occurs, to distribute the requests equally between all the active nodes. When one server is down, the requests are rerouted to the others. When used in a database context, there is only one database for the outside world. It is not specified how this is achieved: database servers can for example share the same data on (high availability) disks if the internal data structures are designed for concurrency. The advantage of active-active is to avoid having a very expensive spare-servers staying idle most of the cases (active-passive configuration). master-master replication (also called multi-master replication) refers to a specific database technique to synchronise database objects across several database instances in a way to ensure global consistency. The advantage is the flexibility, each participating database instance being well encapsulated, and hybrid replication scenarios are possible with several sets of masters. Conclusion: there is a tiny overlap between the two concepts: master-master could be used as one specific way to implement active-active for databases. But active-active can be implemented differently, and multi-master can have other purposes than high availability. AFAIK fail over active active is a kind of a HA (high availability deployment) of a server, where both servers are active (serving requests), via a load balancer (one goes down, the other gets more load) master master replication is a way for a distributed database cluster to have >1 source of truth (multiple master), by keeping the writes in sync using some consensus algo so, the 2 are not entirely the same","title":"Availability"},{"location":"Sort/aws-aurora/","text":"Amazon Aurora As we know it can have up to 15 read-only database I think that these replicas are cache! as i understand they do not keep information about locks.. etc. They just keep pages without lock information. So it looks like complex cache. i mean complex because we ca do some complex logic here: join, group by etc. I guess you may think of read replicas as read-only entity, but that is not true. don't forget about WAL How does WAL impact in case of replica? In Aurora replica does not do any writes? For instance - update. you have to lock an entity before an update, right? Yes. I have to set lock byte in particular row. But only master will do it. Replica does not do any writes to a storage. How a data appears in read replica if so? In case if raft/paxos replicas do writes to log(wal). But in case if aurora replica only do reads. By writes I mean synchronization (in a wide sense) with primary. How aurora does that? Aurora replica just know that its time to reconstract page from storage. as i understand master will send just an event that replica should apply new changes from distributed storage. The main idea that replica does not do any writes ! It looks like cache! I am not sure about sharding. But aurora support multi-master. Can you send an article for that? I kind of agree that read replica mostly required for 2 things: failover read queries that can tolerate eventual consistency but I didn't get the way how the data is transferred from primary to that replicas YT AWS re:Invent 2018: [REPEAT 1] Deep Dive on Amazon Aurora with MySQL Compatibility (DAT304-R1) p1041-verbitski Its also clea why they propose only 15 read-only replicas. It because all replica have the same data. They do not propose sharding on case of single master. Please correct me if i am wrong. If we want to have sharding we need to have multi-master","title":"Amazon Aurora"},{"location":"Sort/aws-aurora/#amazon-aurora","text":"As we know it can have up to 15 read-only database I think that these replicas are cache! as i understand they do not keep information about locks.. etc. They just keep pages without lock information. So it looks like complex cache. i mean complex because we ca do some complex logic here: join, group by etc. I guess you may think of read replicas as read-only entity, but that is not true. don't forget about WAL How does WAL impact in case of replica? In Aurora replica does not do any writes? For instance - update. you have to lock an entity before an update, right? Yes. I have to set lock byte in particular row. But only master will do it. Replica does not do any writes to a storage. How a data appears in read replica if so? In case if raft/paxos replicas do writes to log(wal). But in case if aurora replica only do reads. By writes I mean synchronization (in a wide sense) with primary. How aurora does that? Aurora replica just know that its time to reconstract page from storage. as i understand master will send just an event that replica should apply new changes from distributed storage. The main idea that replica does not do any writes ! It looks like cache! I am not sure about sharding. But aurora support multi-master. Can you send an article for that? I kind of agree that read replica mostly required for 2 things: failover read queries that can tolerate eventual consistency but I didn't get the way how the data is transferred from primary to that replicas YT AWS re:Invent 2018: [REPEAT 1] Deep Dive on Amazon Aurora with MySQL Compatibility (DAT304-R1) p1041-verbitski Its also clea why they propose only 15 read-only replicas. It because all replica have the same data. They do not propose sharding on case of single master. Please correct me if i am wrong. If we want to have sharding we need to have multi-master","title":"Amazon Aurora"},{"location":"Sort/backend-vs-frontend/","text":"Backend vs Frontend Backend is more complex as compared to front end. While designing and developing backend you are supposed to understand other touch points also like data engineering and data analysis, complex architectures which involves multiple backend services calling differnet services either in chain or is mesh, scalling these backend architecture is itself a challenge. Every day new backedlnd frameworks are coming which have at least one strong opinion over other for a use case, same applies with diff databases with each type of use case and interbal designs. The way i see is backend systems amd component has no limit and life long learning opportunity and hence good pay. On the other hand front end is relatively easy. Front end also becomes comples but only at scale. Front end has majir impact on business no doubt but as an engineer i dont see it anywhete near backend. All i see is front end is nothing either mobile, smart devices or browser hence I feel its limitation in terms of os and device. Though I am full stack engineer but I always focus and prefer backend and only take part in front end only where there are no resources (happens rarely and only in case of startup \ud83d\ude02) I feel newbies should target full stack with 80-20 ratio of backend and front end respectively. After couple of years find ur niche and focus on that for next sprint of years. Frontend is very underrated. Good frontend is really complex and hard. To do a properly done UI is an art form. They're both hard to do really well, but in totally different ways. Backend is difficult from a hard technical perspective. You need to really know the ideas and theory behind it. Frontend is hard from a creative perspective. Making a really solid UI work well takes a good sense of design and creativity. Generally frontend engineers are highly responsible for letting UX know what is possible. They shape design and not just simply implement things. It's a core part of the design process to always include frontend engineers. UX and frontend work as a team to produce the best possible UI. It requires a fair amount of creativity to say \"well, this won't work, but we could do x, y, or z in a performant way\" We recommend our UI/UX team even to undergo a basic training involving latest components library based on stack(react) choice like bootstrap, ant design, material. This reduces a lot of time of rework and designers proposes much feasible design based on choice of org's stack. We have seen signifacnt amount of productivity growth in terms of UI/UX and Frond end team when they both understand the universe at ground level. Its just front end guys is more into logical level and can help with experimental feasiblity analysis better because he is in sync with the tech always. This training does not expect any coding from UI/UX team but just understanding of ecosystem of components so that they can propose reusable components based design. Really good UX people know a bit of code too. To call the frontend people out on their BS.","title":"Backend vs Frontend"},{"location":"Sort/backend-vs-frontend/#backend-vs-frontend","text":"Backend is more complex as compared to front end. While designing and developing backend you are supposed to understand other touch points also like data engineering and data analysis, complex architectures which involves multiple backend services calling differnet services either in chain or is mesh, scalling these backend architecture is itself a challenge. Every day new backedlnd frameworks are coming which have at least one strong opinion over other for a use case, same applies with diff databases with each type of use case and interbal designs. The way i see is backend systems amd component has no limit and life long learning opportunity and hence good pay. On the other hand front end is relatively easy. Front end also becomes comples but only at scale. Front end has majir impact on business no doubt but as an engineer i dont see it anywhete near backend. All i see is front end is nothing either mobile, smart devices or browser hence I feel its limitation in terms of os and device. Though I am full stack engineer but I always focus and prefer backend and only take part in front end only where there are no resources (happens rarely and only in case of startup \ud83d\ude02) I feel newbies should target full stack with 80-20 ratio of backend and front end respectively. After couple of years find ur niche and focus on that for next sprint of years. Frontend is very underrated. Good frontend is really complex and hard. To do a properly done UI is an art form. They're both hard to do really well, but in totally different ways. Backend is difficult from a hard technical perspective. You need to really know the ideas and theory behind it. Frontend is hard from a creative perspective. Making a really solid UI work well takes a good sense of design and creativity. Generally frontend engineers are highly responsible for letting UX know what is possible. They shape design and not just simply implement things. It's a core part of the design process to always include frontend engineers. UX and frontend work as a team to produce the best possible UI. It requires a fair amount of creativity to say \"well, this won't work, but we could do x, y, or z in a performant way\" We recommend our UI/UX team even to undergo a basic training involving latest components library based on stack(react) choice like bootstrap, ant design, material. This reduces a lot of time of rework and designers proposes much feasible design based on choice of org's stack. We have seen signifacnt amount of productivity growth in terms of UI/UX and Frond end team when they both understand the universe at ground level. Its just front end guys is more into logical level and can help with experimental feasiblity analysis better because he is in sync with the tech always. This training does not expect any coding from UI/UX team but just understanding of ecosystem of components so that they can propose reusable components based design. Really good UX people know a bit of code too. To call the frontend people out on their BS.","title":"Backend vs Frontend"},{"location":"Sort/behavioral/","text":"Behavioral Tech leads would still need skills around overcoming blockers. So the interview is going to focus on your ability to work through ambiguity, critical thinking around partnerships and ownership mindset, and leadership/mentorship capabilities. This will be more situational and behavioral questions like \"Tell me about a time...\" or \"Say you're in this situation...\" All things you can do before when you anticipate issues: Get stakeholders' buy-in before even beginning the project, especially from a high-level exec. Discovery conversations to uncover shared goals Regular check-ins with stakeholders and email updates reporting progress on KPIs and current blockers On your team, assign an owner to each external dependency About issues that pop up mid project: Tips For each example, choose the example with the biggest scope. The more senior the role, the bigger the scope had better be. Critical thinking about partnerships Choose an example of working with a partner and achieving a positive outcome Give examples in which you successfully delivered something Format: go to partner team, give info on business impact, and align on a shared goal. If can't get buy-in, escalate quickly Ambiguity Times in which you did not know the path forward. What'd you do and how'd you arrive at the solution? Making judgment calls Talk to different people and try to get dissenting opinions: What is wrong with this? Ownership Identify a problem, your role in it, and also highlight your ability to divvy up work amongst others and get people to work well Go into the alarms/checks you built to be alerted of issues How you ensured what you put in place works in the long-term How did you verify success? Why was this problem hard? How did you ensure success? What were your learnings? If you were put in the same situation again, would you have done the same thing? Examples Helped my team learn how to do spikes properly, how to prioritize high-risk tasks first, how to think of 90/10 solutions (clever ways to get the same thing done in 10% of the time), distributed systems, security, how to take feedback (\"feedback is a gift\"), how finding product-market fit works, how to do user interviews, how to assess trade-offs, ... Other Resources TPM Behavioral Questions Someone's compiled notes here Product Management Andrew Chen's website Lenny Rachitsky - article: how to get into product management and thrive StellarPeers - some interesting cases 80 Articles and Books that will Make you a Great Product Manager The Ultimate Guide to Resources for Product Managers Quora What are the best resources/courses for becoming a better product manager?","title":"Behavioral"},{"location":"Sort/behavioral/#behavioral","text":"Tech leads would still need skills around overcoming blockers. So the interview is going to focus on your ability to work through ambiguity, critical thinking around partnerships and ownership mindset, and leadership/mentorship capabilities. This will be more situational and behavioral questions like \"Tell me about a time...\" or \"Say you're in this situation...\" All things you can do before when you anticipate issues: Get stakeholders' buy-in before even beginning the project, especially from a high-level exec. Discovery conversations to uncover shared goals Regular check-ins with stakeholders and email updates reporting progress on KPIs and current blockers On your team, assign an owner to each external dependency About issues that pop up mid project:","title":"Behavioral"},{"location":"Sort/behavioral/#tips","text":"For each example, choose the example with the biggest scope. The more senior the role, the bigger the scope had better be.","title":"Tips"},{"location":"Sort/behavioral/#critical-thinking-about-partnerships","text":"Choose an example of working with a partner and achieving a positive outcome Give examples in which you successfully delivered something Format: go to partner team, give info on business impact, and align on a shared goal. If can't get buy-in, escalate quickly","title":"Critical thinking about partnerships"},{"location":"Sort/behavioral/#ambiguity","text":"Times in which you did not know the path forward. What'd you do and how'd you arrive at the solution? Making judgment calls Talk to different people and try to get dissenting opinions: What is wrong with this?","title":"Ambiguity"},{"location":"Sort/behavioral/#ownership","text":"Identify a problem, your role in it, and also highlight your ability to divvy up work amongst others and get people to work well Go into the alarms/checks you built to be alerted of issues How you ensured what you put in place works in the long-term How did you verify success? Why was this problem hard? How did you ensure success? What were your learnings? If you were put in the same situation again, would you have done the same thing?","title":"Ownership"},{"location":"Sort/behavioral/#examples","text":"Helped my team learn how to do spikes properly, how to prioritize high-risk tasks first, how to think of 90/10 solutions (clever ways to get the same thing done in 10% of the time), distributed systems, security, how to take feedback (\"feedback is a gift\"), how finding product-market fit works, how to do user interviews, how to assess trade-offs, ...","title":"Examples"},{"location":"Sort/behavioral/#other-resources","text":"TPM Behavioral Questions Someone's compiled notes here Product Management Andrew Chen's website Lenny Rachitsky - article: how to get into product management and thrive StellarPeers - some interesting cases 80 Articles and Books that will Make you a Great Product Manager The Ultimate Guide to Resources for Product Managers Quora What are the best resources/courses for becoming a better product manager?","title":"Other Resources"},{"location":"Sort/counters-in-large-scale-systems/","text":"Counters in Large scale systems What nowadays is being used for counters in the world of large scale systems? I see that cassandra has a special thing for it. https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useCounters.html Heard negative reviews about this. Can u just use Redis for counter? Redis has a crdt based counter if eventual consistency is acceptable. It has to be persisted anyway. how would you do this part? We do not have anything except G-Counter for leader less replication in case of eventually consistency. Not sure, but for me it seems like not that often used procedure. Scenario of usage: number of likes in FB The \"like\" counter problem is pretty common, and takes many forms. Youtube video likes, thumbs up, etc. Generally there's a few patterns. Redis is common used, if you simply want to store/return the literal likes. Think of the FB badge embed. What about Redis internals? is it simple long value or something more? Should just be redis hash by user id or post id. Complex key/value. Another idea is to actually create something in the DB, a literal table, which is eventually consistent, that contains like count for a given post or user or whatever. The advantage of this you actually have persistent data that is query-able, as in it's a real database, but it's more complex than using redis. Another another idea would be a top k lambda architecture or spark/flink/batch processing. There are trade offs here as well, but that's more for a real time analytics system such as google analytics. It might help to understand exactly where this counter is being used as that will shape design. In all these cases we do not need to merge counts from enother hosts, do we? i mean we never have case when 2 different hosts show different numbers. Nah posts are resolved to a single cluster, which should be eventually consistent but containing all the data to that post (or whatever). Data is sharded. Does it mean that in this case we have all data in one DC? i mean if we use single cluster to support youtube counters. it could be too much for 1 DC, could not be? There are probably a lot of data even for counters in case of youtube. Pretty old stats: 5B views per day . 57K per second. if each view event size = 1kb we have around 57Mb/sec. 57K/sec is not a big deal but it's better to use several DC because of availability. I mean even DC could be down. so i still think it's better to have multi-master in different DC in that case. Similar to multi master, we can have one topic for each dc in kafka and let each dc consume records from other dcs and on a write update its local counter and emit messages to other dc topics. And btw - views count is kind of financial data. Based on this authors are being paid. What options do we have here except G-counter? if we have sinlge leader we can simply apply click stream. i agree that we do not need GCounter in this case. if we have more than 1 leader we will have to merge counters from different leaders. if we have to merge we need something like G-counter( Because G-Counter suppose merge operation). Talking about multi-master architecture. Design the feature to count the view count for a given news article Interview Question : Suppose you have logs for all users view history. If a user views same article > 1 times within 30 minutes, it will be only counted for one time. Should focus on architecture solutions, scalability etc. My initial impression is to use MapReduce jobs for logs generated for each hour, then aggregate view counts for each hour. In case of MR you will have significant delay in count update. Any better alternative? If we don't care about duplicates, redis could be a very easy and scabale solution, using key as article Id and an integer as a counter. But if we want to apply this custom logic, then it won't work, as we need to look into past data. If we want to store counter for each individual user, then how to aggregate stats from all users? I would propouse to use event stream in case of single node. If we have more than 1 node i would propose also use G-Counter. \"A G-Counter is a increment-only counter (inspired by vector clocks) in which only increment and merge are possible. Incrementing the counter adds 1 to the count for the current node. Divergent histories are resolved by taking the maximum count for each node (like a vector clock merge). The value of the counter is the sum of all node counts.\" How is the sla and how much is the accuracy of the system? This is a typical lambda architecture . For dedup in streaming you need to use checkpoint and watermark. Search for them. I assume SLA is more important than accuracy.","title":"Counters in Large scale systems"},{"location":"Sort/counters-in-large-scale-systems/#counters-in-large-scale-systems","text":"What nowadays is being used for counters in the world of large scale systems? I see that cassandra has a special thing for it. https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useCounters.html Heard negative reviews about this. Can u just use Redis for counter? Redis has a crdt based counter if eventual consistency is acceptable. It has to be persisted anyway. how would you do this part? We do not have anything except G-Counter for leader less replication in case of eventually consistency. Not sure, but for me it seems like not that often used procedure. Scenario of usage: number of likes in FB The \"like\" counter problem is pretty common, and takes many forms. Youtube video likes, thumbs up, etc. Generally there's a few patterns. Redis is common used, if you simply want to store/return the literal likes. Think of the FB badge embed. What about Redis internals? is it simple long value or something more? Should just be redis hash by user id or post id. Complex key/value. Another idea is to actually create something in the DB, a literal table, which is eventually consistent, that contains like count for a given post or user or whatever. The advantage of this you actually have persistent data that is query-able, as in it's a real database, but it's more complex than using redis. Another another idea would be a top k lambda architecture or spark/flink/batch processing. There are trade offs here as well, but that's more for a real time analytics system such as google analytics. It might help to understand exactly where this counter is being used as that will shape design. In all these cases we do not need to merge counts from enother hosts, do we? i mean we never have case when 2 different hosts show different numbers. Nah posts are resolved to a single cluster, which should be eventually consistent but containing all the data to that post (or whatever). Data is sharded. Does it mean that in this case we have all data in one DC? i mean if we use single cluster to support youtube counters. it could be too much for 1 DC, could not be? There are probably a lot of data even for counters in case of youtube. Pretty old stats: 5B views per day . 57K per second. if each view event size = 1kb we have around 57Mb/sec. 57K/sec is not a big deal but it's better to use several DC because of availability. I mean even DC could be down. so i still think it's better to have multi-master in different DC in that case. Similar to multi master, we can have one topic for each dc in kafka and let each dc consume records from other dcs and on a write update its local counter and emit messages to other dc topics. And btw - views count is kind of financial data. Based on this authors are being paid. What options do we have here except G-counter? if we have sinlge leader we can simply apply click stream. i agree that we do not need GCounter in this case. if we have more than 1 leader we will have to merge counters from different leaders. if we have to merge we need something like G-counter( Because G-Counter suppose merge operation). Talking about multi-master architecture.","title":"Counters in Large scale systems"},{"location":"Sort/counters-in-large-scale-systems/#design-the-feature-to-count-the-view-count-for-a-given-news-article","text":"Interview Question : Suppose you have logs for all users view history. If a user views same article > 1 times within 30 minutes, it will be only counted for one time. Should focus on architecture solutions, scalability etc. My initial impression is to use MapReduce jobs for logs generated for each hour, then aggregate view counts for each hour. In case of MR you will have significant delay in count update. Any better alternative? If we don't care about duplicates, redis could be a very easy and scabale solution, using key as article Id and an integer as a counter. But if we want to apply this custom logic, then it won't work, as we need to look into past data. If we want to store counter for each individual user, then how to aggregate stats from all users? I would propouse to use event stream in case of single node. If we have more than 1 node i would propose also use G-Counter. \"A G-Counter is a increment-only counter (inspired by vector clocks) in which only increment and merge are possible. Incrementing the counter adds 1 to the count for the current node. Divergent histories are resolved by taking the maximum count for each node (like a vector clock merge). The value of the counter is the sum of all node counts.\" How is the sla and how much is the accuracy of the system? This is a typical lambda architecture . For dedup in streaming you need to use checkpoint and watermark. Search for them. I assume SLA is more important than accuracy.","title":"Design the feature to count the view count for a given news article"},{"location":"Sort/databases/","text":"Databases It doesn't makes sense to talk about database (rdbms vs nosql) if we dont know QPS . If we have less than 10k-20k/sec write requests we can use even not sharded rdbms. If we have to support 100k/sec write requests it makes sense to start talk about noSql. RDBMS vs Columnar vs KV https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview Dzone articles are really good at these. You can learn a lot from there about the types of databases, when to use realtime or graph or kv databases and much more YT Introduction to NoSQL \u2022 Martin Fowler \u2022 GOTO 2012 Sharding https://vitess.io/docs/15.0/reference/features/sharding/ - for MySQL Shard Size how much is typically the max or recommended size of each shard of say MySQL or for PostgreSQL ? This will help me estimate the number of shards required given that say I need to store 1 PB of data. The link below in \"Data Sharing\" claims that the avg shard size ranges from 20 to 40 GB. Perhaps this is for MySQL. https://www.codercrunch.com/design/634265/designing-instagram#mcetoc_1dv10vl8s1l Replication Why replicate data? improving availability enabling disaster recovery improve performance, leveraging Read Replicas keep data geographically close to the user, e.g., CDN Index An index is an auxiliary data structure that speeds up look-ups by a particular column. As a mental model (not completely accurate), think of taking a particular column of a table and building a balanced BST from the values of that column so you can look each one up in O(log n) time. And imagine each node in the tree has a pointer to the corresponding row in the table. For the flexible sharding model , you might imagine it using a timestamp to pick the pertinent sharding strategy or a feature ID. Hinted Handoff & Schemaless Bufffered Writes In Cassandra's hintedhandoff or Schemaless's Buffered writes - How is the case for master failure before replicating the data to a quorum of nodes handled ? Since these systems are designed to be max write available they cannot discard writes (like in High-water mark). Lets take a specific example - Will Write 4 be lost in case of hintedhandoff (see Log Truncation section here). My hypothesis is that : master failure -> writes are still accepted and written to secondary master -> till master is back up (or another is elected) and then new master replays these writes from secondary so everything is up to date. Only caveat : Immediate writes may not be read available. High-Water Mark Million Records report Let's say there is a table which has millions of records and records get updated frequently in that table. If you had to build a report for end users to show the statics of each hour, what would be your approach. Keep performance in mind since table has huge number of records. It can be done in may ways. Each will have its own pros and cons: Complex Indexing with tradeoff in write performance and less extensible in case any change is needed in report due to cost of reindexing. Create multiple replicas and serve report by any replica only. This might work but if read txn is not implemented optimally may lead to replication lag and can affect the overall perf. View in db, this wont be managable and less extensible. For critical db , views are usually not appreciated. Send the metrics to timeseries monitoring db like prometheus. Not sure if this is right use case for prometheus and may require metrics to be published explicity from write path or prom will have read from slave. Both the ways addition of code is making it less modular in my view. These days DBs can emit change event in the form of stream in async. We can enable these stream of updates. Write a consumer for these updates stream of event. Patch the update in some cold storage and let the user design amd customize their own report in cold storage or write an api for report powered by cold storage. This approach will not touch any code in write path hence no performance degradation in write path and events are sent async so no read performace degragation of db. All we might need is to scale the DB config as per event stream requirement. Other References How Discord stores billions of messages How Discord indexes billions of messages YT Types of NoSQL Databases | Why you should learn NoSQL before your next System Design Interview YT AWS re:Invent 2018: Amazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database (DAT321) https://www.sqlite.org/fasterthanfs.html Index Merge Optimization A deeper dive into Facebook's MySQL 8.0 migration Scaling Datastores at Slack with Vitess - actually a terribly written article Scalability Philosophy Sharding Citusdata - equivalent of Vitess for Postgres Building Distributed Locks with the DynamoDB Lock Client The Architecture of Schemaless, Uber Engineering's Trip Datastore Using MySQL Search Engine Indexing Secrets of ClickHouse Query Performance - ClickHouse is an open-source column-oriented DBMS (columnar database management system) for online analytical processing (OLAP) Apache Parquet: Parquet file internals and inspecting Parquet file structure - a free and open-source column-oriented data storage format.","title":"Databases"},{"location":"Sort/databases/#databases","text":"It doesn't makes sense to talk about database (rdbms vs nosql) if we dont know QPS . If we have less than 10k-20k/sec write requests we can use even not sharded rdbms. If we have to support 100k/sec write requests it makes sense to start talk about noSql. RDBMS vs Columnar vs KV https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview Dzone articles are really good at these. You can learn a lot from there about the types of databases, when to use realtime or graph or kv databases and much more YT Introduction to NoSQL \u2022 Martin Fowler \u2022 GOTO 2012","title":"Databases"},{"location":"Sort/databases/#sharding","text":"https://vitess.io/docs/15.0/reference/features/sharding/ - for MySQL","title":"Sharding"},{"location":"Sort/databases/#shard-size","text":"how much is typically the max or recommended size of each shard of say MySQL or for PostgreSQL ? This will help me estimate the number of shards required given that say I need to store 1 PB of data. The link below in \"Data Sharing\" claims that the avg shard size ranges from 20 to 40 GB. Perhaps this is for MySQL. https://www.codercrunch.com/design/634265/designing-instagram#mcetoc_1dv10vl8s1l","title":"Shard Size"},{"location":"Sort/databases/#replication","text":"Why replicate data? improving availability enabling disaster recovery improve performance, leveraging Read Replicas keep data geographically close to the user, e.g., CDN","title":"Replication"},{"location":"Sort/databases/#index","text":"An index is an auxiliary data structure that speeds up look-ups by a particular column. As a mental model (not completely accurate), think of taking a particular column of a table and building a balanced BST from the values of that column so you can look each one up in O(log n) time. And imagine each node in the tree has a pointer to the corresponding row in the table. For the flexible sharding model , you might imagine it using a timestamp to pick the pertinent sharding strategy or a feature ID.","title":"Index"},{"location":"Sort/databases/#hinted-handoff-schemaless-bufffered-writes","text":"In Cassandra's hintedhandoff or Schemaless's Buffered writes - How is the case for master failure before replicating the data to a quorum of nodes handled ? Since these systems are designed to be max write available they cannot discard writes (like in High-water mark). Lets take a specific example - Will Write 4 be lost in case of hintedhandoff (see Log Truncation section here). My hypothesis is that : master failure -> writes are still accepted and written to secondary master -> till master is back up (or another is elected) and then new master replays these writes from secondary so everything is up to date. Only caveat : Immediate writes may not be read available. High-Water Mark","title":"Hinted Handoff &amp; Schemaless Bufffered Writes"},{"location":"Sort/databases/#million-records-report","text":"Let's say there is a table which has millions of records and records get updated frequently in that table. If you had to build a report for end users to show the statics of each hour, what would be your approach. Keep performance in mind since table has huge number of records. It can be done in may ways. Each will have its own pros and cons: Complex Indexing with tradeoff in write performance and less extensible in case any change is needed in report due to cost of reindexing. Create multiple replicas and serve report by any replica only. This might work but if read txn is not implemented optimally may lead to replication lag and can affect the overall perf. View in db, this wont be managable and less extensible. For critical db , views are usually not appreciated. Send the metrics to timeseries monitoring db like prometheus. Not sure if this is right use case for prometheus and may require metrics to be published explicity from write path or prom will have read from slave. Both the ways addition of code is making it less modular in my view. These days DBs can emit change event in the form of stream in async. We can enable these stream of updates. Write a consumer for these updates stream of event. Patch the update in some cold storage and let the user design amd customize their own report in cold storage or write an api for report powered by cold storage. This approach will not touch any code in write path hence no performance degradation in write path and events are sent async so no read performace degragation of db. All we might need is to scale the DB config as per event stream requirement.","title":"Million Records report"},{"location":"Sort/databases/#other-references","text":"How Discord stores billions of messages How Discord indexes billions of messages YT Types of NoSQL Databases | Why you should learn NoSQL before your next System Design Interview YT AWS re:Invent 2018: Amazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database (DAT321) https://www.sqlite.org/fasterthanfs.html Index Merge Optimization A deeper dive into Facebook's MySQL 8.0 migration Scaling Datastores at Slack with Vitess - actually a terribly written article Scalability Philosophy Sharding Citusdata - equivalent of Vitess for Postgres Building Distributed Locks with the DynamoDB Lock Client The Architecture of Schemaless, Uber Engineering's Trip Datastore Using MySQL Search Engine Indexing Secrets of ClickHouse Query Performance - ClickHouse is an open-source column-oriented DBMS (columnar database management system) for online analytical processing (OLAP) Apache Parquet: Parquet file internals and inspecting Parquet file structure - a free and open-source column-oriented data storage format.","title":"Other References"},{"location":"Sort/design-alert-system/","text":"Design an Alert System Build an alert system in which there was a stream of orderEvents. The orderEvents had different statuses like places/picked/completed. After the order was placed the should create an alert if time period of P seconds had passed but the order was still not completed. P was Inputted into the system. (Saw this question in leetcode). The ask of the problem is to send out alert when P seconds have passed after order was placed but we did not receive picked/completed event. What is the problem? if order events are delivered to the same host (by hash of orderId), that means that all updates are also here. so every time you have a start of the transaction - you put that to a structure something like indexed queue with a time in future when we want alert to be triggered. so next time you have or completed event - remove that from the ds or time passes (you have a job that runs every second) and take all the events that have ts > now. Lets just say we have 100s of 1000s of records which would need to deleted from the data store so that (ts > now) returns only the orders not yet completed...Wouldnt it be an overkill? Somehow we are trying to make a stream driven events into a batch processing events. On every second, if we see that timestamp of data events present in our ds is greater than current timestamp, we would need to send out alerts...We would also need to delete those records from our ds so that the same records are not included in the next second results....The number of records that need to be deleted could be huge One way to think of this problem is to inject auxiliary checkpoint data events containing (orderId, checkPointType) into the stream (after P seconds of order event being received)...In this way, on the consumer end once we receive this type of event, we can check if the order has been completed...If not, then we can send alerts pertaining to that order individually instead of batch processing of events I would say that solution depends on order numbers. In simple case we just need a queue with message status. I mean message status shoud be Waiting during N seconds and became Ready after N seconds. The queue should let do deque only Ready messages. As soon as we deque we can check order status. I am sure that oracle queue have such possibility. I dont know if its possible to build something similar in kafka.i mean kafka does not support wait in queue until particular time point. As far as i remember oracle creates a time based index in order to support this functionality. I am thinking about system design of such distributed queue which support message delay like oracle:\" A message enqueued with delay set will be in the WAITING state, when the delay expires the messages goes to the READY state. DELAY processing requires the queue monitor to be started. Note that delay is set by the producer who enqueues the message. NO_DELAY : the message is available for immediate dequeuing\". But really in the context of an alert, latency lag doesn't matter. Really 100ms for something that is basically an async ping that says \"hey something is up\" is fine. That level of accuracy is probably not required, although you're correct to say that's a product requirement. The alerting stuff is usually best effort, and you don't need to be milisecond accurate. Even for tools like grafana, data dog, etc they are certainly not that accurate. If you waited for P duration, where P is significantly greater than the miliseconds delay, it is usually not a problem.","title":"Design an Alert System"},{"location":"Sort/design-alert-system/#design-an-alert-system","text":"Build an alert system in which there was a stream of orderEvents. The orderEvents had different statuses like places/picked/completed. After the order was placed the should create an alert if time period of P seconds had passed but the order was still not completed. P was Inputted into the system. (Saw this question in leetcode). The ask of the problem is to send out alert when P seconds have passed after order was placed but we did not receive picked/completed event. What is the problem? if order events are delivered to the same host (by hash of orderId), that means that all updates are also here. so every time you have a start of the transaction - you put that to a structure something like indexed queue with a time in future when we want alert to be triggered. so next time you have or completed event - remove that from the ds or time passes (you have a job that runs every second) and take all the events that have ts > now. Lets just say we have 100s of 1000s of records which would need to deleted from the data store so that (ts > now) returns only the orders not yet completed...Wouldnt it be an overkill? Somehow we are trying to make a stream driven events into a batch processing events. On every second, if we see that timestamp of data events present in our ds is greater than current timestamp, we would need to send out alerts...We would also need to delete those records from our ds so that the same records are not included in the next second results....The number of records that need to be deleted could be huge One way to think of this problem is to inject auxiliary checkpoint data events containing (orderId, checkPointType) into the stream (after P seconds of order event being received)...In this way, on the consumer end once we receive this type of event, we can check if the order has been completed...If not, then we can send alerts pertaining to that order individually instead of batch processing of events I would say that solution depends on order numbers. In simple case we just need a queue with message status. I mean message status shoud be Waiting during N seconds and became Ready after N seconds. The queue should let do deque only Ready messages. As soon as we deque we can check order status. I am sure that oracle queue have such possibility. I dont know if its possible to build something similar in kafka.i mean kafka does not support wait in queue until particular time point. As far as i remember oracle creates a time based index in order to support this functionality. I am thinking about system design of such distributed queue which support message delay like oracle:\" A message enqueued with delay set will be in the WAITING state, when the delay expires the messages goes to the READY state. DELAY processing requires the queue monitor to be started. Note that delay is set by the producer who enqueues the message. NO_DELAY : the message is available for immediate dequeuing\". But really in the context of an alert, latency lag doesn't matter. Really 100ms for something that is basically an async ping that says \"hey something is up\" is fine. That level of accuracy is probably not required, although you're correct to say that's a product requirement. The alerting stuff is usually best effort, and you don't need to be milisecond accurate. Even for tools like grafana, data dog, etc they are certainly not that accurate. If you waited for P duration, where P is significantly greater than the miliseconds delay, it is usually not a problem.","title":"Design an Alert System"},{"location":"Sort/design-an-application/","text":"Design an application Design a basic architecture design for an application which can downloads instagram posts store in a database ( fields like post url, timestamp, likes) show them in a web app to user All sync apis. Have to download whole instagram timeline for a given user. Assume an api already exists, which gives batches of images. Proposed to have a microservice which downloads the posts backed by mongodb or ddb. Have a graphql layer to show those posts and have them filter over a web app. Take a look how twitter designed their app. the point that is that the feed usually has to be precomputed already in a majority of case. Celebrities has a different access pattern. I would not think of Mongo or even a no-sql to begin with. I'd gather more requirements first and build a system that scales 3-5x of that. Think of scrapers, caching, DNS load balancing, parse bots, storage. When it gets to fast read write stores, the discussion moves ahead. Sounds like a crawler There is just an additional component to show downloaded content to a user. As i understan the application have to download all posts which were created before today. As for me the main question here is how to find out next post and how to parallelize it.","title":"Design an application"},{"location":"Sort/design-an-application/#design-an-application","text":"Design a basic architecture design for an application which can downloads instagram posts store in a database ( fields like post url, timestamp, likes) show them in a web app to user All sync apis. Have to download whole instagram timeline for a given user. Assume an api already exists, which gives batches of images. Proposed to have a microservice which downloads the posts backed by mongodb or ddb. Have a graphql layer to show those posts and have them filter over a web app. Take a look how twitter designed their app. the point that is that the feed usually has to be precomputed already in a majority of case. Celebrities has a different access pattern. I would not think of Mongo or even a no-sql to begin with. I'd gather more requirements first and build a system that scales 3-5x of that. Think of scrapers, caching, DNS load balancing, parse bots, storage. When it gets to fast read write stores, the discussion moves ahead. Sounds like a crawler There is just an additional component to show downloaded content to a user. As i understan the application have to download all posts which were created before today. As for me the main question here is how to find out next post and how to parallelize it.","title":"Design an application"},{"location":"Sort/design-chat-messaging/","text":"Design Chat Messaging For a messaging application, how does a client send and receive messages? The naive solution is to send messages using POSTs and to receive messages using GETs (polling). For each send, there is a TCP handshake to set up a new connection. For receives, in addition to the aforesaid TCP handshakes, many GETs will return no results. And even when there are new messages, the client has to wait until the next GET to get them. To solve both of these problems, we can use WebSockets. Send latency is improved because we remove the overhead of a TCP handshake per send. And receive latency is improved additionally because clients are pushed new messages as soon as they become available. Long polling is another alternative for receiving messages. To handle failure, such as Internet or the power going out, persistently queue up messages on the client, and if there is a failure, re-queue them in the new WebSocket connection. So that we don\u2019t deliver sent messages twice or display messages more than once, we can give each message a unique, random ID that is generated on the client to achieve idempotence. For Messenger, the read/write ratio is about the same, so it makes sense to use one type of server, which minimizes deployment/maintenance complexity. For something like Slack, there are many more receives than sends, so it may make sense to split up the functionality into read and write services so that they can be scaled separately and optimally for their specific workload characteristics. A WebSocket connection is between a client and a particular API server. So the load balancer must forward packets to the same API server. This can be done with L3 load balancing, which operates at the IP layer (as opposed to the TCP layer). For example, the load balancer can hash the client\u2019s IP address to the same server. Instead of doing that, however, we should use consistent hashing so that we don\u2019t remap all clients to different API servers when an API server goes down, which would kill every WebSocket connection. To minimise handshakes, you can use TCP keep-alive. FB Chat Architecture Question related to cache. A cache is typically used to support more reads and reduce writes to the DB. So in this case, a cache may be introduced for all active user inboxes. When two people are chatting, their inboxes are in the cache. Messages are written to both their inboxes and then transmitted to the client. In this strategy, what is the best way to periodically persist the messages to durable storage? How can we be sure that messages will not be lost before they are written to DB? Generally write through caches take care of writing to DB. Caches can't reduce writes to the DB, or shouldn't be used in this way. The cache should reflect the source of truth, and not be another source of truth. Otherwise, you run into weird sync issues. Ideally, you write aside or like Uday mentioned, write through. fwiw, most caching is a best effort thing, and should be used to reduce the reads, but shouldn't be entirely relied on to be functioning. Cache misses are a thing. Probably what you were thinking of is write behind (write back) caching where we write to the cache first, then write to the DB later. But this doesn't reduce out total DB writes, it just defers them and makes it async. But at the risk of data loss. The question you need to ask is what is the cost of a cache miss? If it's high, write through is preferable because it will synchronously write to cache and DB at the same time. If it's low, cache aside or read through is preferable. They do not need cache for this part of system. Channel Servers take care of message queuing and delivery. Erlang keeps inbox in memory already. May be it can also persist it. i do not know how they garantee durability. How can we be sure that messages will not be lost before they are written to DB? One thing you can do is put the message in a pub/sub topic. Once it's in there, it's persisted. There can be subscribers of that pub/sub topic that write to the DB and separate subscribers that bypass the DB and send the message straight to the recipient. If it\u2019s in-memory, won\u2019t messages get lost if machine gets rebooted or there is a power failure? There's no way they don't store this to disk. There's no magic here, this is probably a poorly written article that is omitting some details. It looks like what they do is flush to disk using Iris. The recent messages are sent from Iris\u2019s memory and the older conversations are fetched from the traditional storage. Iris is built on top of MySQL & Flash memory. How to scale websockets So that say max they r able to scale it out as 100k connections per machine. Instead of each thread for a client they use akka framework. Create an actor for each client. They also say using a thread is expensive due to the call stacks and then concurrency issues arise. Where as with akka it's very lightweight and concurrency issues r handled by itself. Moreover they tweek few system settings to increase the max connections. 65k max connection limit is for a client and not server. Hence for a server we may scale it to anything but due to memory limitations there is a Max cap u can reach to. So 100K is something which is easily doable with 16Gb heap assigned. Message Ordering How is correct message ordering is preserved in a messaging app? Multiple participants sending messages parallely from different devices to your application cannot easily agree on the same order (that's the nature of distributed systems). There are sophisticated algorithms to solve this very problem. But a messaging platform's core requirement should not address this problem because real time interaction takes higher priority. What it can control is that the ordering for each user remains intact. That it does by timestamp + sequence number on the server which has received the message. This way your experience for both users separately remains consistent even if they open the app on multiple devices. There could be different approaches. For instance telegram and whatsApp uses opposite approaches to re-order messages at client side. You can check it. For example . You can switch off your internet and send some messages in Telegam group and whatsApp group. wait enougth time to be sure to have new messages in both groups. you will see a difference. if we are talking about whatsApp,Discord .. These use actor model. Each group has it own actor. This actor defines total ordering. So you have total order within single group and not in whole messages. But ideally in an interview if the ordering logic is asked them the interviewer would be more interested to see what the candidates approach is towards ordering. Whether it is lamports or vector clocks and why. It's better for us to discuss about trade-off in different approaches. for example if we know 2 different approaches we can \"compare\" these approaches during an interview and point out what are props and cons of these different approaches in case of particular requirements. Twitter IDs Akka Motivation behind Actors - As for me the main idea of actor = lightweight thread + inbound queue. Each host can have a lot of actors (much more compare to number of CPU). Actors send messages to each other( all messages will be placed to inboud queue before processing).if we have an inbound queue for messages we have an ordering and dont have contention . of course there more details .. for example - supervision, actor hierarchy Other References Facebook Real-time Chat Architecture Scaling With Over Multi-Billion Messages Daily YT MySQL for Messaging - @Scale 2014 - Data Meta Building Mobile-First Infrastructure for Messenger Slideshare Storage Infrastructure Behind Facebook Messages - Info on the persistence tech in fb, I find the presentation helpful to understand the storage layer https://labs.ripe.net/author/ramakrishna_padmanabhan/reasons-dynamic-addresses-change/ - if the IP address changes then a new WebSocket connection will need to be established. WS connections dropping is pretty normal. Once a new connection is established, the client can indicate what message it received last and it can send messages that weren't sent yet.","title":"Design Chat Messaging"},{"location":"Sort/design-chat-messaging/#design-chat-messaging","text":"For a messaging application, how does a client send and receive messages? The naive solution is to send messages using POSTs and to receive messages using GETs (polling). For each send, there is a TCP handshake to set up a new connection. For receives, in addition to the aforesaid TCP handshakes, many GETs will return no results. And even when there are new messages, the client has to wait until the next GET to get them. To solve both of these problems, we can use WebSockets. Send latency is improved because we remove the overhead of a TCP handshake per send. And receive latency is improved additionally because clients are pushed new messages as soon as they become available. Long polling is another alternative for receiving messages. To handle failure, such as Internet or the power going out, persistently queue up messages on the client, and if there is a failure, re-queue them in the new WebSocket connection. So that we don\u2019t deliver sent messages twice or display messages more than once, we can give each message a unique, random ID that is generated on the client to achieve idempotence. For Messenger, the read/write ratio is about the same, so it makes sense to use one type of server, which minimizes deployment/maintenance complexity. For something like Slack, there are many more receives than sends, so it may make sense to split up the functionality into read and write services so that they can be scaled separately and optimally for their specific workload characteristics. A WebSocket connection is between a client and a particular API server. So the load balancer must forward packets to the same API server. This can be done with L3 load balancing, which operates at the IP layer (as opposed to the TCP layer). For example, the load balancer can hash the client\u2019s IP address to the same server. Instead of doing that, however, we should use consistent hashing so that we don\u2019t remap all clients to different API servers when an API server goes down, which would kill every WebSocket connection. To minimise handshakes, you can use TCP keep-alive.","title":"Design Chat Messaging"},{"location":"Sort/design-chat-messaging/#fb-chat-architecture","text":"Question related to cache. A cache is typically used to support more reads and reduce writes to the DB. So in this case, a cache may be introduced for all active user inboxes. When two people are chatting, their inboxes are in the cache. Messages are written to both their inboxes and then transmitted to the client. In this strategy, what is the best way to periodically persist the messages to durable storage? How can we be sure that messages will not be lost before they are written to DB? Generally write through caches take care of writing to DB. Caches can't reduce writes to the DB, or shouldn't be used in this way. The cache should reflect the source of truth, and not be another source of truth. Otherwise, you run into weird sync issues. Ideally, you write aside or like Uday mentioned, write through. fwiw, most caching is a best effort thing, and should be used to reduce the reads, but shouldn't be entirely relied on to be functioning. Cache misses are a thing. Probably what you were thinking of is write behind (write back) caching where we write to the cache first, then write to the DB later. But this doesn't reduce out total DB writes, it just defers them and makes it async. But at the risk of data loss. The question you need to ask is what is the cost of a cache miss? If it's high, write through is preferable because it will synchronously write to cache and DB at the same time. If it's low, cache aside or read through is preferable. They do not need cache for this part of system. Channel Servers take care of message queuing and delivery. Erlang keeps inbox in memory already. May be it can also persist it. i do not know how they garantee durability. How can we be sure that messages will not be lost before they are written to DB? One thing you can do is put the message in a pub/sub topic. Once it's in there, it's persisted. There can be subscribers of that pub/sub topic that write to the DB and separate subscribers that bypass the DB and send the message straight to the recipient. If it\u2019s in-memory, won\u2019t messages get lost if machine gets rebooted or there is a power failure? There's no way they don't store this to disk. There's no magic here, this is probably a poorly written article that is omitting some details. It looks like what they do is flush to disk using Iris. The recent messages are sent from Iris\u2019s memory and the older conversations are fetched from the traditional storage. Iris is built on top of MySQL & Flash memory.","title":"FB Chat Architecture"},{"location":"Sort/design-chat-messaging/#how-to-scale-websockets","text":"So that say max they r able to scale it out as 100k connections per machine. Instead of each thread for a client they use akka framework. Create an actor for each client. They also say using a thread is expensive due to the call stacks and then concurrency issues arise. Where as with akka it's very lightweight and concurrency issues r handled by itself. Moreover they tweek few system settings to increase the max connections. 65k max connection limit is for a client and not server. Hence for a server we may scale it to anything but due to memory limitations there is a Max cap u can reach to. So 100K is something which is easily doable with 16Gb heap assigned.","title":"How to scale websockets"},{"location":"Sort/design-chat-messaging/#message-ordering","text":"How is correct message ordering is preserved in a messaging app? Multiple participants sending messages parallely from different devices to your application cannot easily agree on the same order (that's the nature of distributed systems). There are sophisticated algorithms to solve this very problem. But a messaging platform's core requirement should not address this problem because real time interaction takes higher priority. What it can control is that the ordering for each user remains intact. That it does by timestamp + sequence number on the server which has received the message. This way your experience for both users separately remains consistent even if they open the app on multiple devices. There could be different approaches. For instance telegram and whatsApp uses opposite approaches to re-order messages at client side. You can check it. For example . You can switch off your internet and send some messages in Telegam group and whatsApp group. wait enougth time to be sure to have new messages in both groups. you will see a difference. if we are talking about whatsApp,Discord .. These use actor model. Each group has it own actor. This actor defines total ordering. So you have total order within single group and not in whole messages. But ideally in an interview if the ordering logic is asked them the interviewer would be more interested to see what the candidates approach is towards ordering. Whether it is lamports or vector clocks and why. It's better for us to discuss about trade-off in different approaches. for example if we know 2 different approaches we can \"compare\" these approaches during an interview and point out what are props and cons of these different approaches in case of particular requirements. Twitter IDs Akka Motivation behind Actors - As for me the main idea of actor = lightweight thread + inbound queue. Each host can have a lot of actors (much more compare to number of CPU). Actors send messages to each other( all messages will be placed to inboud queue before processing).if we have an inbound queue for messages we have an ordering and dont have contention . of course there more details .. for example - supervision, actor hierarchy","title":"Message Ordering"},{"location":"Sort/design-chat-messaging/#other-references","text":"Facebook Real-time Chat Architecture Scaling With Over Multi-Billion Messages Daily YT MySQL for Messaging - @Scale 2014 - Data Meta Building Mobile-First Infrastructure for Messenger Slideshare Storage Infrastructure Behind Facebook Messages - Info on the persistence tech in fb, I find the presentation helpful to understand the storage layer https://labs.ripe.net/author/ramakrishna_padmanabhan/reasons-dynamic-addresses-change/ - if the IP address changes then a new WebSocket connection will need to be established. WS connections dropping is pretty normal. Once a new connection is established, the client can indicate what message it received last and it can send messages that weren't sent yet.","title":"Other References"},{"location":"Sort/design-chess/","text":"Design Chess One is to think chess board as an object instead of data structure. Similar approach was used in pacman game when developers were stuck on smell issue - where pacman/player keeps a trail or 'smell' and those devils keep on following the smell. When they modeled the smell to be property of pacman/player, it was getting extremely hard to keep track of it due player turning left/right etc. So they added smell as property of grid/board. Similarly, we can use chessboard as an object and can keep track of cells which are under threat etc. - this makes very easy to find if king is in check, or, also to figure out move which will put king into check. Secondly, I agree with tree and bfs approach. Only thing is - since a particular move can be made after 1,2,3 next moves, or even repeated (everything except pawn can go back to previous place) - instead of n-ary tree, this becomes directed acyclic graph. In fact, actual chess engines work almost like this. And it is also right that number of moves go higher exponentially (not exactly sure, but i heard there are some million possible ways just at move 8-10 or even earlier). And this is the reason why those chess engines rely on opening books. Thus, for first few moves, they don't have to generate graphs with millions nodes (i.e. moves). As the game progresses and pieces are captured, graph starts to become smaller. And then there comes value to each piece - this way the graph can be made even smaller. E.g. if I make a move which puts other player's queen under attack, then it is almost safe to assume that either other player will make a move to eliminate that threat, or I can capture the queen at next move. Either way, lot of possibilities are reduced this way. Of course, it is very rudimentary approach, because seasoned players go for positional play where they'll gladly sacrifice the queen if they can find forced mate, or some other forced combination which wins more material - including queen. In fact, there are grandmaster level games where queen was sacrificed, and player actually played 8-10 or even more moves and won. But approaches suggested by me and others above should be good enough for chess engine with nominal depth. Finally, if you make a move on a piece such that your own king is checked without enemy moving anything. That's the crux. Engine needs to detect that and flag the move invalid. This is the part that changed my entire design. That's where we need to attach behavior to board so that board is aware which cells are under threat (or can be threatened immediately). Moving a king to a cell which is currently under a threat is illegal move. Moving any other piece to such cell is legal, but maybe not recommended move.","title":"Design Chess"},{"location":"Sort/design-chess/#design-chess","text":"One is to think chess board as an object instead of data structure. Similar approach was used in pacman game when developers were stuck on smell issue - where pacman/player keeps a trail or 'smell' and those devils keep on following the smell. When they modeled the smell to be property of pacman/player, it was getting extremely hard to keep track of it due player turning left/right etc. So they added smell as property of grid/board. Similarly, we can use chessboard as an object and can keep track of cells which are under threat etc. - this makes very easy to find if king is in check, or, also to figure out move which will put king into check. Secondly, I agree with tree and bfs approach. Only thing is - since a particular move can be made after 1,2,3 next moves, or even repeated (everything except pawn can go back to previous place) - instead of n-ary tree, this becomes directed acyclic graph. In fact, actual chess engines work almost like this. And it is also right that number of moves go higher exponentially (not exactly sure, but i heard there are some million possible ways just at move 8-10 or even earlier). And this is the reason why those chess engines rely on opening books. Thus, for first few moves, they don't have to generate graphs with millions nodes (i.e. moves). As the game progresses and pieces are captured, graph starts to become smaller. And then there comes value to each piece - this way the graph can be made even smaller. E.g. if I make a move which puts other player's queen under attack, then it is almost safe to assume that either other player will make a move to eliminate that threat, or I can capture the queen at next move. Either way, lot of possibilities are reduced this way. Of course, it is very rudimentary approach, because seasoned players go for positional play where they'll gladly sacrifice the queen if they can find forced mate, or some other forced combination which wins more material - including queen. In fact, there are grandmaster level games where queen was sacrificed, and player actually played 8-10 or even more moves and won. But approaches suggested by me and others above should be good enough for chess engine with nominal depth. Finally, if you make a move on a piece such that your own king is checked without enemy moving anything. That's the crux. Engine needs to detect that and flag the move invalid. This is the part that changed my entire design. That's where we need to attach behavior to board so that board is aware which cells are under threat (or can be threatened immediately). Moving a king to a cell which is currently under a threat is illegal move. Moving any other piece to such cell is legal, but maybe not recommended move.","title":"Design Chess"},{"location":"Sort/design-db-connector/","text":"Design a DB Connector LLD Question: you have to design a db connector with connection pooling that abstracts away db type (sql, mongo). To instantiate, args aredb type and usual credentials. To run query you provide a run() interface. GitHub - rohillashivam/custom-connection-pooling","title":"Design a DB Connector"},{"location":"Sort/design-db-connector/#design-a-db-connector","text":"LLD Question: you have to design a db connector with connection pooling that abstracts away db type (sql, mongo). To instantiate, args aredb type and usual credentials. To run query you provide a run() interface. GitHub - rohillashivam/custom-connection-pooling","title":"Design a DB Connector"},{"location":"Sort/design-delayed-payment/","text":"Design a system for delayed payment Interview question : Say that if you deposit a check, the check balance will be available after 2 days after deposit. I'd just use priority queues instead of regular fifo for account update events. Prioritized by current time - update time. Partition by account. Each update needs to write to a quorom and have the same transaction id. That way each partition can try to update the account database independently but only the first with the Id is applied. Any fancier ways of doing this? The question is more about how to persist the future events and how to trigger at a big scale. Robinhood has a similar question. Stripe onsite had also a question similar to this one. It's a pretty fun question, because you basically touch every aspect of a large scale system. If you want to nail the question you have to go through how to design a scalable Executor Service and Executor Resource in the article. The article doesn't go through it. evens, queues, executors, data storage, resiliency, fault tolerance, it's just a starting point, but once you have those pieces you can easily break the problem down. The excecutor service is more accurately an excecutor cluster . Every company has a job runner team and setup. Every company needs to push notifications to users or services There's really 3 large parts: execution cluster , triggering/scheduling cluster , and notification clusters : https://slack.engineering/scaling-slacks-job-queue/ - It looks like backpressure https://databricks.com/session/optimal-strategies-for-large-scale-batch-etl-jobs https://www.alibabacloud.com/blog/alibaba-core-scheduling-system-job-scheduler-2-0---meeting-big-data-and-cloud-computing-scheduling-challenges_596557 Depositing a check is just \"as fast as we can verify. The reason the bank takes 2 days to process isn't some artificial delay, there's compliance checks and various regulations banks need to adhere to. So in the processing of a check, it's more like a series of checkpoints and not some kind of delayed event trigger system. SAGA architecture would work well here, where each step is gated by a service and transaction, which can be event driven. Actually the interviewer told me that they hold the check for 2 days, and release the fund even if the validation with the other bank is not completed, to make customer happy. I think this is how the banks work and thats how most of the check frauds happen in US. They credit the amount to the account and if one has transferred from that amount and the check is found to be bogus the account holder is liable. It not clear why we need 2 days for each transaction. May its just for some specific cases. References If we're looking for enterprise scale event triggering, quartz scheduler is a well known solution https://medium.com/javarevisited/how-to-cluster-effectively-quartz-jobs-9b097f5e1191","title":"Design a system for delayed payment"},{"location":"Sort/design-delayed-payment/#design-a-system-for-delayed-payment","text":"Interview question : Say that if you deposit a check, the check balance will be available after 2 days after deposit. I'd just use priority queues instead of regular fifo for account update events. Prioritized by current time - update time. Partition by account. Each update needs to write to a quorom and have the same transaction id. That way each partition can try to update the account database independently but only the first with the Id is applied. Any fancier ways of doing this? The question is more about how to persist the future events and how to trigger at a big scale. Robinhood has a similar question. Stripe onsite had also a question similar to this one. It's a pretty fun question, because you basically touch every aspect of a large scale system. If you want to nail the question you have to go through how to design a scalable Executor Service and Executor Resource in the article. The article doesn't go through it. evens, queues, executors, data storage, resiliency, fault tolerance, it's just a starting point, but once you have those pieces you can easily break the problem down. The excecutor service is more accurately an excecutor cluster . Every company has a job runner team and setup. Every company needs to push notifications to users or services There's really 3 large parts: execution cluster , triggering/scheduling cluster , and notification clusters : https://slack.engineering/scaling-slacks-job-queue/ - It looks like backpressure https://databricks.com/session/optimal-strategies-for-large-scale-batch-etl-jobs https://www.alibabacloud.com/blog/alibaba-core-scheduling-system-job-scheduler-2-0---meeting-big-data-and-cloud-computing-scheduling-challenges_596557 Depositing a check is just \"as fast as we can verify. The reason the bank takes 2 days to process isn't some artificial delay, there's compliance checks and various regulations banks need to adhere to. So in the processing of a check, it's more like a series of checkpoints and not some kind of delayed event trigger system. SAGA architecture would work well here, where each step is gated by a service and transaction, which can be event driven. Actually the interviewer told me that they hold the check for 2 days, and release the fund even if the validation with the other bank is not completed, to make customer happy. I think this is how the banks work and thats how most of the check frauds happen in US. They credit the amount to the account and if one has transferred from that amount and the check is found to be bogus the account holder is liable. It not clear why we need 2 days for each transaction. May its just for some specific cases.","title":"Design a system for delayed payment"},{"location":"Sort/design-delayed-payment/#references","text":"If we're looking for enterprise scale event triggering, quartz scheduler is a well known solution https://medium.com/javarevisited/how-to-cluster-effectively-quartz-jobs-9b097f5e1191","title":"References"},{"location":"Sort/design-distributed-queue/","text":"Design distributed queue How would you design distributed queue that support one billion topics? I think its a good question because it shows if designer understands \"limitations\" and can orove that solution can support 1 billuon topics. why is this a good interview question? seems like it would require very specific knowledge about kafka and how to scale kafka We can not scale kafka for 1 billion topics. Kafka has a limitation because of controller. I mean one of kafka broker is a controller .this controller is responsible for assigment a leader for each partition. If that controller killed ( died) it will take a lot of time to recover from that in case of 1 billion topic/partitions. So kafka approach can not be used for 1 billion. There is an article that kafka can support up to 200k partitions only. So we need more scalable architecture. kafka can do more than 200k partitions now, they removed zookeeper. it can do millions. but still not a great question because it assumes really deep knowledge of streaming system internals. this is like a question you'd ask someone who designs streaming services, like a core kafka contributor or something. kafka does not keep offset in zookeper now. since ZK is needed to save state, it becomes the bottleneck in exchanging metadata, so by removing ZK, you remove the bottleneck to the traditionally held 200k partitions. They were able to reach 200k because of async update of zk. I mean it used to be sequentional before kafka 1.1.0. ZK is the problem, having this central bottleneck is not scalable. controller will not let scale well. Also kafka create many files for each topic. May be 5 files or something like this. We can not have 5 billion files https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/ https://www.confluent.io/blog/apache-kafka-supports-200k-partitions-per-cluster/","title":"Design distributed queue"},{"location":"Sort/design-distributed-queue/#design-distributed-queue","text":"How would you design distributed queue that support one billion topics? I think its a good question because it shows if designer understands \"limitations\" and can orove that solution can support 1 billuon topics. why is this a good interview question? seems like it would require very specific knowledge about kafka and how to scale kafka We can not scale kafka for 1 billion topics. Kafka has a limitation because of controller. I mean one of kafka broker is a controller .this controller is responsible for assigment a leader for each partition. If that controller killed ( died) it will take a lot of time to recover from that in case of 1 billion topic/partitions. So kafka approach can not be used for 1 billion. There is an article that kafka can support up to 200k partitions only. So we need more scalable architecture. kafka can do more than 200k partitions now, they removed zookeeper. it can do millions. but still not a great question because it assumes really deep knowledge of streaming system internals. this is like a question you'd ask someone who designs streaming services, like a core kafka contributor or something. kafka does not keep offset in zookeper now. since ZK is needed to save state, it becomes the bottleneck in exchanging metadata, so by removing ZK, you remove the bottleneck to the traditionally held 200k partitions. They were able to reach 200k because of async update of zk. I mean it used to be sequentional before kafka 1.1.0. ZK is the problem, having this central bottleneck is not scalable. controller will not let scale well. Also kafka create many files for each topic. May be 5 files or something like this. We can not have 5 billion files https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/ https://www.confluent.io/blog/apache-kafka-supports-200k-partitions-per-cluster/","title":"Design distributed queue"},{"location":"Sort/design-email-system/","text":"Design Email System design an email sending system: how can you guarantee the emails are sent to recipents. Even in the case of server failures, it should send them when server come back up. How would you guys design it? Message Queue with acknowledgements? It would be best to understand how IMAP and POP3 work to answer this effectively, I believe. These are very mature protocols which probably address a lot of issues out of the box. SMTP for message delivery, POP/IMAP for message fetch/store. MEssage Transfer Agent (MTA) servers are based around queues for different senders, and messages are stored and forwarded as and when they come. For server failues just have replica servers, nothing else.","title":"Design Email System"},{"location":"Sort/design-email-system/#design-email-system","text":"design an email sending system: how can you guarantee the emails are sent to recipents. Even in the case of server failures, it should send them when server come back up. How would you guys design it? Message Queue with acknowledgements? It would be best to understand how IMAP and POP3 work to answer this effectively, I believe. These are very mature protocols which probably address a lot of issues out of the box. SMTP for message delivery, POP/IMAP for message fetch/store. MEssage Transfer Agent (MTA) servers are based around queues for different senders, and messages are stored and forwarded as and when they come. For server failues just have replica servers, nothing else.","title":"Design Email System"},{"location":"Sort/design-google-doc/","text":"Design a Google Doc system Designing a Google Doc system with focusing on searching through the Documents. It seems the interviewer was not happy with him choosing the ElasticSearch for indexing. Elastic search might be an overkill. If you just need exact search, inverted index should be enough. The interviewer does not wanted you to design the scalable inverted index itself. Spending time building a search engine from scratch is a separate interview. Maybe they wanted you to explain the pros and cons of different engine types. Maybe ElasticSearch is too generic/easy for these types of questions and you kinda need to implement your own indexing architecture. ES probably works fine for millions of documents but not hundreds of millions. So if you're looking for a solution for everyone at google scale, you probably need map reduce + inverted index generation. There are limits to what ES can do efficiently and it's not a universal solution. Remember: A system design interview isn\u2019t about giving a rote correct answer. It\u2019s about showing your knowledge , discussing trade offs, explaining the why. For timeline search i also used elastic search, i had follow up question on how u will be storing data and more. And feedback came as E4 may be because by using elastic search I abstracted details of how data is going be stored and scaling scenarios. But now I think I would have said inverted index, i would have given more details by myself to interviewer than he asking me, which might would have lead to different results. You can say elastic search as long as you explain how it works. How it stores data. Common issues with the design decisions ES might have made. Etc. You can name drop an existing project as long as you can explain the ins and outs and pitfalls of it. You cant just say elastic search and move on without details as if it\u2019s a quiz about who can name the most technologies to solve this design puzzle. Instagram initially had Elastisearch and then they moved to FB Unicorn .","title":"Design a Google Doc system"},{"location":"Sort/design-google-doc/#design-a-google-doc-system","text":"Designing a Google Doc system with focusing on searching through the Documents. It seems the interviewer was not happy with him choosing the ElasticSearch for indexing. Elastic search might be an overkill. If you just need exact search, inverted index should be enough. The interviewer does not wanted you to design the scalable inverted index itself. Spending time building a search engine from scratch is a separate interview. Maybe they wanted you to explain the pros and cons of different engine types. Maybe ElasticSearch is too generic/easy for these types of questions and you kinda need to implement your own indexing architecture. ES probably works fine for millions of documents but not hundreds of millions. So if you're looking for a solution for everyone at google scale, you probably need map reduce + inverted index generation. There are limits to what ES can do efficiently and it's not a universal solution. Remember: A system design interview isn\u2019t about giving a rote correct answer. It\u2019s about showing your knowledge , discussing trade offs, explaining the why. For timeline search i also used elastic search, i had follow up question on how u will be storing data and more. And feedback came as E4 may be because by using elastic search I abstracted details of how data is going be stored and scaling scenarios. But now I think I would have said inverted index, i would have given more details by myself to interviewer than he asking me, which might would have lead to different results. You can say elastic search as long as you explain how it works. How it stores data. Common issues with the design decisions ES might have made. Etc. You can name drop an existing project as long as you can explain the ins and outs and pitfalls of it. You cant just say elastic search and move on without details as if it\u2019s a quiz about who can name the most technologies to solve this design puzzle. Instagram initially had Elastisearch and then they moved to FB Unicorn .","title":"Design a Google Doc system"},{"location":"Sort/design-log-transporter/","text":"Design Read 1Mb log files from client machines In real world, you write log files out locally, and some task or process reads through them line by line, chunking/batching it and send it off as a request. You then catch it in a distributed queue and process into whatever form you want. Imagine \"design splunk\" or \"design datadog\". The conceptual idea is logs as event streams. The log on the client is then rotated or deleted by the service itself. This is a nice pattern because the log is append only and therefore very cheap, and allows you to write huge volumes of logs. The process that reads those logs only needs to understand that, and can run as another container aside your application. Separate resourcing too. If the log processor running locally crashes or has problems, you can just replay it back on the last found timestamp to get your logs back. If the app crashes, any unsent logs can be sent, and then allow the container to terminate. As you send batches of logs, you infuse that batch with metadata. So app name, container, etc etc. Batch -> Compress -> (maybe) stream Stream stuff is arguable, it's better to chunk and send in small fragments, overhead of maintaining the stream and stuff is complicated. Then if you're missing a fragment, you can just ask for that chunk. Imagine running millions of container or something. Keeping all those streams open is expensive. Better to be async. Batches (configurable) should be big enough to take advantage of compression. Like greater than 2mb for gzip.","title":"Design Read 1Mb log files from client machines"},{"location":"Sort/design-log-transporter/#design-read-1mb-log-files-from-client-machines","text":"In real world, you write log files out locally, and some task or process reads through them line by line, chunking/batching it and send it off as a request. You then catch it in a distributed queue and process into whatever form you want. Imagine \"design splunk\" or \"design datadog\". The conceptual idea is logs as event streams. The log on the client is then rotated or deleted by the service itself. This is a nice pattern because the log is append only and therefore very cheap, and allows you to write huge volumes of logs. The process that reads those logs only needs to understand that, and can run as another container aside your application. Separate resourcing too. If the log processor running locally crashes or has problems, you can just replay it back on the last found timestamp to get your logs back. If the app crashes, any unsent logs can be sent, and then allow the container to terminate. As you send batches of logs, you infuse that batch with metadata. So app name, container, etc etc. Batch -> Compress -> (maybe) stream Stream stuff is arguable, it's better to chunk and send in small fragments, overhead of maintaining the stream and stuff is complicated. Then if you're missing a fragment, you can just ask for that chunk. Imagine running millions of container or something. Keeping all those streams open is expensive. Better to be async. Batches (configurable) should be big enough to take advantage of compression. Like greater than 2mb for gzip.","title":"Design Read 1Mb log files from client machines"},{"location":"Sort/design-real-time-collab/","text":"Design Real-time collab tool Other Resources https://youtu.be/jIR0Ngov7vo https://youtu.be/hy0ePbpna5Y YT - Building a collaborative text editor with WebRTC and CRDTs","title":"Design Real-time collab tool"},{"location":"Sort/design-real-time-collab/#design-real-time-collab-tool","text":"","title":"Design Real-time collab tool"},{"location":"Sort/design-real-time-collab/#other-resources","text":"https://youtu.be/jIR0Ngov7vo https://youtu.be/hy0ePbpna5Y YT - Building a collaborative text editor with WebRTC and CRDTs","title":"Other Resources"},{"location":"Sort/design-stock-exchange/","text":"Design a Stock Exchange Price of security can change every microsecond or nano second, technically it's not bound to time, but bid/offer match. And so the trigger not bounded to time if price reaches/crosses the threshold, action should be taken. When price change, we can create an event that will take action on gtt priority queue similar to bid and offer priority queues. Also the price change is dependent on bid and offer match events. And we can not have delay for triggering order as small delay can cause order not getting executed later (on same price there could be millions orders), orders on same price follow fifo) If we have a requirement that latency should as fast as possible, we can do the following: have a server that subscribes to the real time market data on this server have some number of such rules and also have a subscription for updates of such rules in this server for each symbol have a RB Tree of > when the update comes to the system, check the head of this rb tree, remove orderIds that is less that current price send those orderIds to the execution YT Stock Exchange System Design YT How to Build an Exchange YT - Building Low Latency Trading Systems T - Event Sourcing & CQRS | Stock Exchange Microservices Architecture | System Design Primer Side Thoughts i don't think that any real stock exchange system uses an ACID database (to register orders), Kafka( as a service bus), because of not an appropriate performance for this case. I hear that KDB and Aaron are used by the real stock exchange system. Regarding performance requirements - \"At the turn of the 21st century, HFT trades had an execution time of several seconds, whereas by 2010 this had decreased to milli- and even microseconds\". So we need to have microseconds for end-to-end latency. Zerodha - Zerodha's scale and tech stack. He kind of left in middle and didn't share exact tech used to handle ~16mn ticks/sec","title":"Design a Stock Exchange"},{"location":"Sort/design-stock-exchange/#design-a-stock-exchange","text":"Price of security can change every microsecond or nano second, technically it's not bound to time, but bid/offer match. And so the trigger not bounded to time if price reaches/crosses the threshold, action should be taken. When price change, we can create an event that will take action on gtt priority queue similar to bid and offer priority queues. Also the price change is dependent on bid and offer match events. And we can not have delay for triggering order as small delay can cause order not getting executed later (on same price there could be millions orders), orders on same price follow fifo) If we have a requirement that latency should as fast as possible, we can do the following: have a server that subscribes to the real time market data on this server have some number of such rules and also have a subscription for updates of such rules in this server for each symbol have a RB Tree of > when the update comes to the system, check the head of this rb tree, remove orderIds that is less that current price send those orderIds to the execution YT Stock Exchange System Design YT How to Build an Exchange YT - Building Low Latency Trading Systems T - Event Sourcing & CQRS | Stock Exchange Microservices Architecture | System Design Primer","title":"Design a Stock Exchange"},{"location":"Sort/design-stock-exchange/#side-thoughts","text":"i don't think that any real stock exchange system uses an ACID database (to register orders), Kafka( as a service bus), because of not an appropriate performance for this case. I hear that KDB and Aaron are used by the real stock exchange system. Regarding performance requirements - \"At the turn of the 21st century, HFT trades had an execution time of several seconds, whereas by 2010 this had decreased to milli- and even microseconds\". So we need to have microseconds for end-to-end latency. Zerodha - Zerodha's scale and tech stack. He kind of left in middle and didn't share exact tech used to handle ~16mn ticks/sec","title":"Side Thoughts"},{"location":"Sort/design-ticket-sales-service/","text":"Design Ticket Sales service Design ticket sales service where there is a spike of sales from time to time. example cases: 90% of customers. small event happening in your city, selling 100 seats and usually around 100 person is interested in more difficult case to scale: Apple wants to use this service to sell tickets for their upcoming conference where there are 50k seats and 1M people are interested in buying another difficult case from business perspective: you have 50k seats, ~60k people are interested. why this is difficult case, lets assume you put everyone in FIFO queue and 10k people are waiting, 50k people started doing checkout process, but 10k of them couldn't finish on time and dropped from checkout (some didn't want, some didn't have enough money), in the meantime, in the waiting queue of 10k people, 5k left getting bored of waiting, now at most you have 45k people attending your event, from business perspective this is loss of profit, which should be minimized. For the 10K waiting part of the problem, i wonder if they do some sort of overbooking similar to airlines. Using previous data you can figure out how many ppl you expect to drop out of the process, and oversell by that amount. Probably statisticians figure this out I guess. Overbooking in airlines is possible because: Seats aren't assigned at the time of booking -- applicable for a conference/ media event There typically are other flights flying that route where you could adjust the overflow, and make alternative arrangements Flights being a large scale operation, there's enough stats of passenger footfall and patterns available to model and optimize the overbooking factor. If there's only 60k people interested in an event that has 50k capacity, it's highly unlikely that there will be a long queue.. unless there's a very small window of time when booking is open. If you have 60k people interested in 50k event this already means there is small window of time for bookings. what I have seen in the past (my peers trying to attend some conferences) for some conferences people are restarting page constantly to start booking when booking window is open, because they will be sold quite fast. If you give 10min for checkout process, this is quite long period, lots of people on waiting queue will leave, if you give 30sec then it might be quite small and a lot of people can't finish checkout You will have deep queues if.. theres a significantly large number of people interested in an event compared to the capacity. Or there's a modeet number of people strongly interested in the event. My assumption in both cases is that you will not have a large number of drops. If people are so excited to get into an event that they will start queing at the first moment of booking opening, rest assured they will not drop out from a 10 minute waiting. From an engineer's stand point, there is still an opportunity for the underlying technology challenges to be discussed, regardless of whether the problem statement is real or hypothetical. Send notification to all registered users after window period. Over charging user and refunding in case tickets were already sold. Seats allocation depends on how fast he/she will book. I think this has 2 problems: Unfair system, you can write bot who can buy all tickets because they can finish faster, in case of if we have queue, still bots can finish faster, but at least humans have chance based on their click of Book button earlier than some bots. for 50k event with 60k interests, refunding might be ok, you don't get lots of complains on Twitter, imagine what happens with Apple conference, 950.000 people get refund and if 5% start complaining, you get network effect of complains and this hurts company image, so next time Apple won't be your customer Tokens Model My thoughts regarding handling spike, I was initially thinking about tokens model. Overview of design: store 2 lists in redis for each event List 1. token per available seat with TTL of time when event starts List 2. tokens where currently someone finishing checkout when book clicked, take token from 1st list, put into other list and return token to user (pop from list and put into other is in transaction, using Lua) when purchase finished. client sends token (or it comes as part of payment provider webhook) token is stored in RDBMS (or any persistent storage), token is deleted from Redis (remember this, we will come back here) if purchase wasn't finished, for users who are still retrying backend takes token from 2nd list if time to finish checkout is expired, assign it to user and return to user, now user can do checkout There are multiple issues with this solution. lets assume token is 24bytes, you have 100k events, on average offering 1k seats. on redis you will have 100M * 24 bytes of data, not much but still to keep in mind if redis crashes (case 1) you should have reliable way of regenerating all these tokens (or enable persistence for redis, get slightly slower performance), consider also what kind of issues might arise if you have enabled redis replication remember this part. if you have already noticed we are relying on payment provider to send us token so we can mark that token is taken, what if there are some issues happening on webhook part of payment provider? your tokens will be used multiple times, effectively selling 1M tickets, which are not available. Not sure if this good solution, just borrowed some ideas from rate limiter, one big difference is rate limiter doesn't need to be accurate all the time, but in bookmyshow case we should be accurate as much as possible.","title":"Design Ticket Sales service"},{"location":"Sort/design-ticket-sales-service/#design-ticket-sales-service","text":"Design ticket sales service where there is a spike of sales from time to time. example cases: 90% of customers. small event happening in your city, selling 100 seats and usually around 100 person is interested in more difficult case to scale: Apple wants to use this service to sell tickets for their upcoming conference where there are 50k seats and 1M people are interested in buying another difficult case from business perspective: you have 50k seats, ~60k people are interested. why this is difficult case, lets assume you put everyone in FIFO queue and 10k people are waiting, 50k people started doing checkout process, but 10k of them couldn't finish on time and dropped from checkout (some didn't want, some didn't have enough money), in the meantime, in the waiting queue of 10k people, 5k left getting bored of waiting, now at most you have 45k people attending your event, from business perspective this is loss of profit, which should be minimized. For the 10K waiting part of the problem, i wonder if they do some sort of overbooking similar to airlines. Using previous data you can figure out how many ppl you expect to drop out of the process, and oversell by that amount. Probably statisticians figure this out I guess. Overbooking in airlines is possible because: Seats aren't assigned at the time of booking -- applicable for a conference/ media event There typically are other flights flying that route where you could adjust the overflow, and make alternative arrangements Flights being a large scale operation, there's enough stats of passenger footfall and patterns available to model and optimize the overbooking factor. If there's only 60k people interested in an event that has 50k capacity, it's highly unlikely that there will be a long queue.. unless there's a very small window of time when booking is open. If you have 60k people interested in 50k event this already means there is small window of time for bookings. what I have seen in the past (my peers trying to attend some conferences) for some conferences people are restarting page constantly to start booking when booking window is open, because they will be sold quite fast. If you give 10min for checkout process, this is quite long period, lots of people on waiting queue will leave, if you give 30sec then it might be quite small and a lot of people can't finish checkout You will have deep queues if.. theres a significantly large number of people interested in an event compared to the capacity. Or there's a modeet number of people strongly interested in the event. My assumption in both cases is that you will not have a large number of drops. If people are so excited to get into an event that they will start queing at the first moment of booking opening, rest assured they will not drop out from a 10 minute waiting. From an engineer's stand point, there is still an opportunity for the underlying technology challenges to be discussed, regardless of whether the problem statement is real or hypothetical. Send notification to all registered users after window period. Over charging user and refunding in case tickets were already sold. Seats allocation depends on how fast he/she will book. I think this has 2 problems: Unfair system, you can write bot who can buy all tickets because they can finish faster, in case of if we have queue, still bots can finish faster, but at least humans have chance based on their click of Book button earlier than some bots. for 50k event with 60k interests, refunding might be ok, you don't get lots of complains on Twitter, imagine what happens with Apple conference, 950.000 people get refund and if 5% start complaining, you get network effect of complains and this hurts company image, so next time Apple won't be your customer","title":"Design Ticket Sales service"},{"location":"Sort/design-ticket-sales-service/#tokens-model","text":"My thoughts regarding handling spike, I was initially thinking about tokens model. Overview of design: store 2 lists in redis for each event List 1. token per available seat with TTL of time when event starts List 2. tokens where currently someone finishing checkout when book clicked, take token from 1st list, put into other list and return token to user (pop from list and put into other is in transaction, using Lua) when purchase finished. client sends token (or it comes as part of payment provider webhook) token is stored in RDBMS (or any persistent storage), token is deleted from Redis (remember this, we will come back here) if purchase wasn't finished, for users who are still retrying backend takes token from 2nd list if time to finish checkout is expired, assign it to user and return to user, now user can do checkout There are multiple issues with this solution. lets assume token is 24bytes, you have 100k events, on average offering 1k seats. on redis you will have 100M * 24 bytes of data, not much but still to keep in mind if redis crashes (case 1) you should have reliable way of regenerating all these tokens (or enable persistence for redis, get slightly slower performance), consider also what kind of issues might arise if you have enabled redis replication remember this part. if you have already noticed we are relying on payment provider to send us token so we can mark that token is taken, what if there are some issues happening on webhook part of payment provider? your tokens will be used multiple times, effectively selling 1M tickets, which are not available. Not sure if this good solution, just borrowed some ideas from rate limiter, one big difference is rate limiter doesn't need to be accurate all the time, but in bookmyshow case we should be accurate as much as possible.","title":"Tokens Model"},{"location":"Sort/design-top-k-frequent/","text":"Design top k frequent Design a service that would give top K most frequent results where the input is an unbounded data stream. Some options: LB partitions the input stream accoss multiple minheap service. The Read API service will ask the top K from each minheap service, merge and return the result. Drawback: storage issue in minheap service, how long to retain the data as the input is infinite. log all the incoming request across different partitions, run MAP-Reduce jobs and then return the top K results Drawback: Map-Reduce is batch jobs. won't get real-time results using some probabilistic DS like count min sketch to get the real-time approximate top K results and combine with option#2 to get more accurate top K results over a longer period. Standard answer is lambda architecture. The real world answer is use a stream processing framework like Spark or Flink. But basically some combination of all those options are needed. Modern stream processing frameworks can actually do map reduce size jobs in addition to short term calculations. I dont think that \"standard\" minHeap will work in option 1, as the K is unknown it could be 5min or 5 days. You need to put pair (word, 1) if its absent and you need to increase frequence if its already there. But if you have such not standard Heap you need to keep TopK for thrumbling windows. Spark streaming will submit a new job for each window. I mean it will batch messages and submit a new job to process each micro batch Definitely there will be an upper bound. let say 1K ? Any major design consideration? In \u0441ase of 1k system can keeps data in memory.if we have 1 billion.... its another case Stay away from heap type implementations, it's problematic for a large variety of reasons. The heap data structure isn't really designed for range queries. Also scaling it out to many instances has issues. So for TopK. i would propose: kafka + spark streaming + keep intermediate results in database ( partition by time) + service which can merge result for different windows (it produces result for range query). Not sure about sharding. I was thinking along the lines of using redis. Processing the incoming data and updating sorted list in redis. Data could be shared based on hash of the word or range. To get top k elements, top k elements from all redis instances could be picked and then merged to return the overall top k. The issue is the immense amount of requests need to materialize a single request. It's not very efficient. Also, most top k require some time function, so top k within 5 min, top k within 15 min, top k per hour, etc. So redis will not be able to easily handle top k over time. One solution is use a MQ like Kafka and feed the data to some Map Reduce functions which would give results in few hours. But if you want information in few minutes use count Min sketch algorithm. The problem with Min Sketch algorithm is you won\u2019t get accurate results. If you want more accurate results use more Hash functions Remote Discover Server is a no-Sql (KV) in memory database. Again Through put is the key here. if during the interview, this term comes up or asked as one of the requirement, Kafka is the way. speed is different and trhoughput is different. Play accordingly. quick read here This is usually called lambda architecture Most of those ideas were lifted from this video . Btw this double system is actually mostly dead in the real world. Everyone just uses Flink or Spark. Most stream processing systems can take the place of map reduce now. It's like the question \"design typeahead\". No one uses Trie, but it's totally accepted. Count Min Sketch How Count Min Sketch will help? if we have upper bound limit 1K we don't need probabilistic structures at all. i mean we will have to use count min sketch in the tumbling windows. Lets say our key is consume 1000 bytes. So we just need 1000 1000 8 (size of key * number of keys * size of value) to keep our structure in memory. What about size of Count Min Sketch for 1K? Searching & Auto-complete When i type \"obama barack\" in youtube search they allso suggest \"barack obama singing\" not \"obama barack singing\". youtube most likely uses ML learning and other complex tools, not ES. So that's a very different system serving results. All the search engines from google use some complex model. What do you think about Trie + Inverted Index for interview ? I mean to propouse use union of Trie + Inverted index i will build Inverted Index only for whole word. i mean that it will keep relation between word and sentences: word -> [sentence1,sentences2,sentence3..] . so if i have a string s1 . i will take a Trie.get(s1) + InvertedIndex.get(s1). Most likely it will return something form Trie for any s1 and it will return from InvertedIndex only if s1 is a word. Why? We get basically nothing out of that other than a more complex design that can do less for us. Now we need to handle trie construction AND inverted re-indexing. Now we need to keep them in sync. As a general rule of thumb, keep things simple, then optimize. ES can do everything a Trie can do, and more. Plus it handles concerns like sharding, scalability, data integrity and supports complex query operations, all things we would need to implement for a Trie. ES even has built-in prefix query support. You don't even need to generate it yourself. https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html Mixing ES and Tries is just no good. Other References YT System Design : Top 10 Songs, Top Trending songs, Top K listed Google: min sketch algorithms","title":"Design top k frequent"},{"location":"Sort/design-top-k-frequent/#design-top-k-frequent","text":"Design a service that would give top K most frequent results where the input is an unbounded data stream. Some options: LB partitions the input stream accoss multiple minheap service. The Read API service will ask the top K from each minheap service, merge and return the result. Drawback: storage issue in minheap service, how long to retain the data as the input is infinite. log all the incoming request across different partitions, run MAP-Reduce jobs and then return the top K results Drawback: Map-Reduce is batch jobs. won't get real-time results using some probabilistic DS like count min sketch to get the real-time approximate top K results and combine with option#2 to get more accurate top K results over a longer period. Standard answer is lambda architecture. The real world answer is use a stream processing framework like Spark or Flink. But basically some combination of all those options are needed. Modern stream processing frameworks can actually do map reduce size jobs in addition to short term calculations. I dont think that \"standard\" minHeap will work in option 1, as the K is unknown it could be 5min or 5 days. You need to put pair (word, 1) if its absent and you need to increase frequence if its already there. But if you have such not standard Heap you need to keep TopK for thrumbling windows. Spark streaming will submit a new job for each window. I mean it will batch messages and submit a new job to process each micro batch Definitely there will be an upper bound. let say 1K ? Any major design consideration? In \u0441ase of 1k system can keeps data in memory.if we have 1 billion.... its another case Stay away from heap type implementations, it's problematic for a large variety of reasons. The heap data structure isn't really designed for range queries. Also scaling it out to many instances has issues. So for TopK. i would propose: kafka + spark streaming + keep intermediate results in database ( partition by time) + service which can merge result for different windows (it produces result for range query). Not sure about sharding. I was thinking along the lines of using redis. Processing the incoming data and updating sorted list in redis. Data could be shared based on hash of the word or range. To get top k elements, top k elements from all redis instances could be picked and then merged to return the overall top k. The issue is the immense amount of requests need to materialize a single request. It's not very efficient. Also, most top k require some time function, so top k within 5 min, top k within 15 min, top k per hour, etc. So redis will not be able to easily handle top k over time. One solution is use a MQ like Kafka and feed the data to some Map Reduce functions which would give results in few hours. But if you want information in few minutes use count Min sketch algorithm. The problem with Min Sketch algorithm is you won\u2019t get accurate results. If you want more accurate results use more Hash functions Remote Discover Server is a no-Sql (KV) in memory database. Again Through put is the key here. if during the interview, this term comes up or asked as one of the requirement, Kafka is the way. speed is different and trhoughput is different. Play accordingly. quick read here This is usually called lambda architecture Most of those ideas were lifted from this video . Btw this double system is actually mostly dead in the real world. Everyone just uses Flink or Spark. Most stream processing systems can take the place of map reduce now. It's like the question \"design typeahead\". No one uses Trie, but it's totally accepted.","title":"Design top k frequent"},{"location":"Sort/design-top-k-frequent/#count-min-sketch","text":"How Count Min Sketch will help? if we have upper bound limit 1K we don't need probabilistic structures at all. i mean we will have to use count min sketch in the tumbling windows. Lets say our key is consume 1000 bytes. So we just need 1000 1000 8 (size of key * number of keys * size of value) to keep our structure in memory. What about size of Count Min Sketch for 1K?","title":"Count Min Sketch"},{"location":"Sort/design-top-k-frequent/#searching-auto-complete","text":"When i type \"obama barack\" in youtube search they allso suggest \"barack obama singing\" not \"obama barack singing\". youtube most likely uses ML learning and other complex tools, not ES. So that's a very different system serving results. All the search engines from google use some complex model. What do you think about Trie + Inverted Index for interview ? I mean to propouse use union of Trie + Inverted index i will build Inverted Index only for whole word. i mean that it will keep relation between word and sentences: word -> [sentence1,sentences2,sentence3..] . so if i have a string s1 . i will take a Trie.get(s1) + InvertedIndex.get(s1). Most likely it will return something form Trie for any s1 and it will return from InvertedIndex only if s1 is a word. Why? We get basically nothing out of that other than a more complex design that can do less for us. Now we need to handle trie construction AND inverted re-indexing. Now we need to keep them in sync. As a general rule of thumb, keep things simple, then optimize. ES can do everything a Trie can do, and more. Plus it handles concerns like sharding, scalability, data integrity and supports complex query operations, all things we would need to implement for a Trie. ES even has built-in prefix query support. You don't even need to generate it yourself. https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html Mixing ES and Tries is just no good.","title":"Searching &amp; Auto-complete"},{"location":"Sort/design-top-k-frequent/#other-references","text":"YT System Design : Top 10 Songs, Top Trending songs, Top K listed Google: min sketch algorithms","title":"Other References"},{"location":"Sort/hashmap-based-caching/","text":"HashMap based caching Simple HashMap based caching in java. I have 4 Maps where the data is stored and there are significantly more reads than writes. When there is an refresh from the db, all the maps are updated together and no read operation can be allowed during this time over any map as the data in one map is related to the other map. Read cannot be allowed during an inconsistent state. What would be the best approach to do tackle this? Using simple HashMaps with ReadWriteLock or synchronisation when updating? Using ConcurrentHashMap? Using volatile maps? Fetching data in temporary mals and then assigning them when the slow db operations are done? How long does the refresh take? Is the read request sync or async? Read can not be allowed.. What is the expected behavior if a read request happens during the reload? Block? Throw error? Solution 1: Multi thread asynchronous processes for read write with thread safety. Solution 2: I would go with last approach (fetching data in temp...), if additional memory is available for those 4 hash maps. You can have 2 sets of these 4 hash maps. Call the first set current set and second set staging set. When you have refresh event, update values in staging set hash maps. Once refresh work is over, Make staging set current set and and old current set as staging set. During this time, you can continue to serve read request from current set, while your refresh event is preparing staging set hash maps. This brings down bottle neck to one flip operation from current set to stage set.","title":"HashMap based caching"},{"location":"Sort/hashmap-based-caching/#hashmap-based-caching","text":"Simple HashMap based caching in java. I have 4 Maps where the data is stored and there are significantly more reads than writes. When there is an refresh from the db, all the maps are updated together and no read operation can be allowed during this time over any map as the data in one map is related to the other map. Read cannot be allowed during an inconsistent state. What would be the best approach to do tackle this? Using simple HashMaps with ReadWriteLock or synchronisation when updating? Using ConcurrentHashMap? Using volatile maps? Fetching data in temporary mals and then assigning them when the slow db operations are done? How long does the refresh take? Is the read request sync or async? Read can not be allowed.. What is the expected behavior if a read request happens during the reload? Block? Throw error? Solution 1: Multi thread asynchronous processes for read write with thread safety. Solution 2: I would go with last approach (fetching data in temp...), if additional memory is available for those 4 hash maps. You can have 2 sets of these 4 hash maps. Call the first set current set and second set staging set. When you have refresh event, update values in staging set hash maps. Once refresh work is over, Make staging set current set and and old current set as staging set. During this time, you can continue to serve read request from current set, while your refresh event is preparing staging set hash maps. This brings down bottle neck to one flip operation from current set to stage set.","title":"HashMap based caching"},{"location":"Sort/interview-experience/","text":"Interview Experience Google L6 What demonstrates L6/L7 vs L5 in terms of system design in the limited time? The bar for L6 is incredibly high at G. (IDK about other FANG). Was told L6 must be able to transform an abstract problem statement into a solution and be able to defend. The part that one should be able to defend is dual edged, it can tick off some interviewers. Where did you do your paid mocks, did you set expectations before the mock? Expertmithra, interviewing.io What was your question? If it was too broad, unless you were able to get E2E for at least workflow consider it downlevel or worse a reject. Auction system. Missed some industry terminology/optimizations Persuasiveness is underrated at L6+ levels. Learnt the hard way from my feedbacks. Agreed, any rubric or targeted guide for this? What do you do to improve here and how do you measure this? I consider myself fairly extroverted and social, but may be I can do better. It's all about likeability, comm & how humans perceive others. Just be yourself Stuck on a problem What's a better strategy for leetcode questions that one is not able to crack after spending a reasonable amount of time? continue to solve it yourself indefinitely, or look up solution, code it, and move on to next problem? This depends on how much time you have. If starting out and have time, I suggest sleeping on it so that patterns emerge ie 1, but if you have a deadline or want to time-bound the prep, makes sense to check after say 10-30 mins and move on as there are so many areas to cover. Regardless, more imp is to attack that problem soon again so that its fresh(spaced repetition). Destination Dispatch Design and code an elevator destination dispatch system where users select their destination outside the elevators, and are assigned an elevator car by the system. A breath of fresh air compared to leetcode-style questions. It\u2019s a pretty common OOP design question Other Resources https://medium.com/@yashgirdhar/11-companies-55-interviews-9-offers-including-google-and-amazon-heres-what-i-have-to-share-293852c1c98f https://www.teamblind.com/post/Giving-back---how-I-cleared-L6-System-Design---Part-1-4yufM3RY https://sirupsen.com/napkin","title":"Interview Experience"},{"location":"Sort/interview-experience/#interview-experience","text":"","title":"Interview Experience"},{"location":"Sort/interview-experience/#google-l6","text":"What demonstrates L6/L7 vs L5 in terms of system design in the limited time? The bar for L6 is incredibly high at G. (IDK about other FANG). Was told L6 must be able to transform an abstract problem statement into a solution and be able to defend. The part that one should be able to defend is dual edged, it can tick off some interviewers. Where did you do your paid mocks, did you set expectations before the mock? Expertmithra, interviewing.io What was your question? If it was too broad, unless you were able to get E2E for at least workflow consider it downlevel or worse a reject. Auction system. Missed some industry terminology/optimizations Persuasiveness is underrated at L6+ levels. Learnt the hard way from my feedbacks. Agreed, any rubric or targeted guide for this? What do you do to improve here and how do you measure this? I consider myself fairly extroverted and social, but may be I can do better. It's all about likeability, comm & how humans perceive others. Just be yourself","title":"Google L6"},{"location":"Sort/interview-experience/#stuck-on-a-problem","text":"What's a better strategy for leetcode questions that one is not able to crack after spending a reasonable amount of time? continue to solve it yourself indefinitely, or look up solution, code it, and move on to next problem? This depends on how much time you have. If starting out and have time, I suggest sleeping on it so that patterns emerge ie 1, but if you have a deadline or want to time-bound the prep, makes sense to check after say 10-30 mins and move on as there are so many areas to cover. Regardless, more imp is to attack that problem soon again so that its fresh(spaced repetition).","title":"Stuck on a problem"},{"location":"Sort/interview-experience/#destination-dispatch","text":"Design and code an elevator destination dispatch system where users select their destination outside the elevators, and are assigned an elevator car by the system. A breath of fresh air compared to leetcode-style questions. It\u2019s a pretty common OOP design question","title":"Destination Dispatch"},{"location":"Sort/interview-experience/#other-resources","text":"https://medium.com/@yashgirdhar/11-companies-55-interviews-9-offers-including-google-and-amazon-heres-what-i-have-to-share-293852c1c98f https://www.teamblind.com/post/Giving-back---how-I-cleared-L6-System-Design---Part-1-4yufM3RY https://sirupsen.com/napkin","title":"Other Resources"},{"location":"Sort/load-balancing/","text":"Load Balancing Consider we have a service behind a load balancer, where we spawn instance of server as load increases . We initially started with N instances of server . As load increases we decide increment server instance to N+1. Assume server was using consistent hashing to distribute load. At this instance , all new connections which are supposed to go to new server will work fine , but what will happen to the old and running connections which were setup prior to spawning instance N+1. Fundamentally , because of consistent hashing only fraction of old connection will move to this new instance , but those old connection had setup their TCP connection with some other server on the consistent hashing ring when number of server instance were N. Is there hand off of TCP connection to this new server instance? Does LB keep track of which server instance it assigned to connection when it saw first packet of the connection ? And keeps track of connection till its closure? Assume these connections were long polling connections. For use cases where a long running session is established between a client and an instance in a server cluster, you don't usually handoff ongoing sessions to new instances. However, you can (depending on use case) have mechanisms for a different instance to take over in case one of them fails. For instance, handoff in a game of chess, or in a shopping cart checkout is easier, compared to let's say an online game of PUBG. Load balancers often use sticky sessions to redirect requests for existing sessions to the same server instance that was originally handling the request (unless it had died, of course). Only new sessions would be routed based on the hashing. This still means that instances already processing at near peak capacity continue to get more workload, and the new instance only get part of the new workload. However, I guess load balancer can have enough buffer beyond thresholds so that this doesn't actually cause problems, or they can actively pick instances with low workload till a fair distribution is reached. I am not exactly sure what strategies are considered best approach to address this. TCP/IP once goes to listening and waiting mode, there is only certain number of them defined by operating system. Sometimes these don\u2019t clear up so we have to explicitly reduce the wait_time to a very low number. If TCP/IP port already picked them up we can not transfer. TCP connections are tied to the machine, if it goes to another machine TCP will reply back RESET. So generally app servers are kept stateless and L7 load balancer takes care of handling the tcp connection. One such example where TCP connection stays with app server is websocket but in that case you use an L4 load balancer which maintains mapping between tcp connections and server machines handling those. So if it's L7 load balancer is the one terminating the connection ... Doesn't it become bottleneck ? Since entry point for all your traffic is your load balancer. I understand we can have L4 load balancer in front of multiple L7 load balancer .... But then we are back to square one \ud83e\udd14 , what happens if increment number of L7 load balancer by 1 ? There is no consistent hashing involved and requests are handled by next layer of web servers. LB acts as an intermediate between client and server. It terminates the connection and forwards the request to web server. LB is rarely a bottleneck in practice. More often your DB or some other part of the system becomes a bottlneck first. What happens when this reconciliation of keys is happening when a new server is getting added or some server fails/crashes? Or rather ... What happens to existing connection? In the L7 LB, where termination happens at the LB itself, the LB will just route requests to known healthy hosts. Any existing connections to the bad instances are forcibly closed. What do you mean by \"reconciliation of keys\"? By reconciliation what i meant was , when we decide to add a new instance of server , fundamentally by consistent hashing it is supposed to own some amount of keys from existing set of keys . Extending this basics to LB , keys here are our existing ongoing connections which were mapped to some server when we had N servers . Post addition of this (n+1)th server, some fraction of connection should have ideally got passed over to this new server . But because of the nature of TCP connection , we just can't simply ask this new server to respond to existing ongoing connections. As others pointed out to something called as sticky sessions , seems possible solution. But then LB seems to become single point of failure or scalability as it needs to maintain sticky session . Other option of LB (layer 7) terminating the TCP connection, looks great and seems to be maintaining consistent view connection . But then I feel is this not again single point of failure or scalability? LB and consistent hashing usually don't go together. So when you run dynamo or something, the DB itself figures out who to talk to and why, not some load balancer. This can be zookeeper, which is fairly common, or some form of quorum or consensus. A LB might front the entire DB cluster, but won't be used to actually route requests to which DB instance contains what key. Most LB just do round robin, random allocation, there are some load based ideas too. Even with sticky sessions, we just allow existing sessions to exist, assign new sticky sessions to new instances as they come in. If an instance dies, that sticky session dies with it, and then the user reconnects and gets assigned another instance. LB are definitely a single point of failure , but they are unlikely to be a bottleneck. For some services you might have a backup LB waiting. Terminating at L7 is usually a somewhat light operation and most systems won't have an issue with it. Perhaps if you get to millions of requests per second you want a L3 (or L4) LB or something, but those are usually gateways to large systems such as an entire cloud provider. TLS termination is work, so when you do L7 LB, you do extra work. Imagine L3/L4 as being pass through where termination happens elsewhere, so you're offloading work to something else (other LB, other services) therefore you can scale up to huge numbers, tens of millions of requests per second. LB generally doesn't do much heavy computation. Its sole job is to redirect requests. Looking up a session in a HashTable is very fast. The LB's network interface is going to be a bottleneck much before the computing power is. And then you look into strategies like Hardware load-balancer, Geography based DNS routing, and multiple IP addresses mapped to the same DNS, thus distributing the load of your load balancer over different instances. Ususally there are routers, L4, L7 load balancers. Routers send traffic to L4 which inturn send traffic to L7. Even if L4 LB goes down generally state is distributed so other L4 LBs should be able take the traffic without any disruption. If L7 goes down L4 LB sends traffic to other L7 LBs and this mapping would be updated at L4 LB layer. Don't see how that addresses the problem. The first point of contact in the LB hierarchy is a unique host. If that goes down, a new first point of contact needs to be established. (and before that, the failure also needs to be discovered) Intuitively thinking, this means the client has to be notified of a new entry point or a standby instance assumes the same address. Both of these mean there is a window of disruption. Consistent Hashing There are two types of consistent hashing algorithms available, Ketama and Wheel. Both types are supported by libmemcached, and implementations are available for PHP and Java. https://docs.oracle.com/cd/E17952_01/mysql-5.6-en/ha-memcached-using-hashtypes.html Other Resources Load Balancing 101: Nuts and Bolts | F5 - has a lot of relevant info, though it doesn't explain what happens when lb fails.","title":"Load Balancing"},{"location":"Sort/load-balancing/#load-balancing","text":"Consider we have a service behind a load balancer, where we spawn instance of server as load increases . We initially started with N instances of server . As load increases we decide increment server instance to N+1. Assume server was using consistent hashing to distribute load. At this instance , all new connections which are supposed to go to new server will work fine , but what will happen to the old and running connections which were setup prior to spawning instance N+1. Fundamentally , because of consistent hashing only fraction of old connection will move to this new instance , but those old connection had setup their TCP connection with some other server on the consistent hashing ring when number of server instance were N. Is there hand off of TCP connection to this new server instance? Does LB keep track of which server instance it assigned to connection when it saw first packet of the connection ? And keeps track of connection till its closure? Assume these connections were long polling connections. For use cases where a long running session is established between a client and an instance in a server cluster, you don't usually handoff ongoing sessions to new instances. However, you can (depending on use case) have mechanisms for a different instance to take over in case one of them fails. For instance, handoff in a game of chess, or in a shopping cart checkout is easier, compared to let's say an online game of PUBG. Load balancers often use sticky sessions to redirect requests for existing sessions to the same server instance that was originally handling the request (unless it had died, of course). Only new sessions would be routed based on the hashing. This still means that instances already processing at near peak capacity continue to get more workload, and the new instance only get part of the new workload. However, I guess load balancer can have enough buffer beyond thresholds so that this doesn't actually cause problems, or they can actively pick instances with low workload till a fair distribution is reached. I am not exactly sure what strategies are considered best approach to address this. TCP/IP once goes to listening and waiting mode, there is only certain number of them defined by operating system. Sometimes these don\u2019t clear up so we have to explicitly reduce the wait_time to a very low number. If TCP/IP port already picked them up we can not transfer. TCP connections are tied to the machine, if it goes to another machine TCP will reply back RESET. So generally app servers are kept stateless and L7 load balancer takes care of handling the tcp connection. One such example where TCP connection stays with app server is websocket but in that case you use an L4 load balancer which maintains mapping between tcp connections and server machines handling those. So if it's L7 load balancer is the one terminating the connection ... Doesn't it become bottleneck ? Since entry point for all your traffic is your load balancer. I understand we can have L4 load balancer in front of multiple L7 load balancer .... But then we are back to square one \ud83e\udd14 , what happens if increment number of L7 load balancer by 1 ? There is no consistent hashing involved and requests are handled by next layer of web servers. LB acts as an intermediate between client and server. It terminates the connection and forwards the request to web server. LB is rarely a bottleneck in practice. More often your DB or some other part of the system becomes a bottlneck first. What happens when this reconciliation of keys is happening when a new server is getting added or some server fails/crashes? Or rather ... What happens to existing connection? In the L7 LB, where termination happens at the LB itself, the LB will just route requests to known healthy hosts. Any existing connections to the bad instances are forcibly closed. What do you mean by \"reconciliation of keys\"? By reconciliation what i meant was , when we decide to add a new instance of server , fundamentally by consistent hashing it is supposed to own some amount of keys from existing set of keys . Extending this basics to LB , keys here are our existing ongoing connections which were mapped to some server when we had N servers . Post addition of this (n+1)th server, some fraction of connection should have ideally got passed over to this new server . But because of the nature of TCP connection , we just can't simply ask this new server to respond to existing ongoing connections. As others pointed out to something called as sticky sessions , seems possible solution. But then LB seems to become single point of failure or scalability as it needs to maintain sticky session . Other option of LB (layer 7) terminating the TCP connection, looks great and seems to be maintaining consistent view connection . But then I feel is this not again single point of failure or scalability? LB and consistent hashing usually don't go together. So when you run dynamo or something, the DB itself figures out who to talk to and why, not some load balancer. This can be zookeeper, which is fairly common, or some form of quorum or consensus. A LB might front the entire DB cluster, but won't be used to actually route requests to which DB instance contains what key. Most LB just do round robin, random allocation, there are some load based ideas too. Even with sticky sessions, we just allow existing sessions to exist, assign new sticky sessions to new instances as they come in. If an instance dies, that sticky session dies with it, and then the user reconnects and gets assigned another instance. LB are definitely a single point of failure , but they are unlikely to be a bottleneck. For some services you might have a backup LB waiting. Terminating at L7 is usually a somewhat light operation and most systems won't have an issue with it. Perhaps if you get to millions of requests per second you want a L3 (or L4) LB or something, but those are usually gateways to large systems such as an entire cloud provider. TLS termination is work, so when you do L7 LB, you do extra work. Imagine L3/L4 as being pass through where termination happens elsewhere, so you're offloading work to something else (other LB, other services) therefore you can scale up to huge numbers, tens of millions of requests per second. LB generally doesn't do much heavy computation. Its sole job is to redirect requests. Looking up a session in a HashTable is very fast. The LB's network interface is going to be a bottleneck much before the computing power is. And then you look into strategies like Hardware load-balancer, Geography based DNS routing, and multiple IP addresses mapped to the same DNS, thus distributing the load of your load balancer over different instances. Ususally there are routers, L4, L7 load balancers. Routers send traffic to L4 which inturn send traffic to L7. Even if L4 LB goes down generally state is distributed so other L4 LBs should be able take the traffic without any disruption. If L7 goes down L4 LB sends traffic to other L7 LBs and this mapping would be updated at L4 LB layer. Don't see how that addresses the problem. The first point of contact in the LB hierarchy is a unique host. If that goes down, a new first point of contact needs to be established. (and before that, the failure also needs to be discovered) Intuitively thinking, this means the client has to be notified of a new entry point or a standby instance assumes the same address. Both of these mean there is a window of disruption.","title":"Load Balancing"},{"location":"Sort/load-balancing/#consistent-hashing","text":"There are two types of consistent hashing algorithms available, Ketama and Wheel. Both types are supported by libmemcached, and implementations are available for PHP and Java. https://docs.oracle.com/cd/E17952_01/mysql-5.6-en/ha-memcached-using-hashtypes.html","title":"Consistent Hashing"},{"location":"Sort/load-balancing/#other-resources","text":"Load Balancing 101: Nuts and Bolts | F5 - has a lot of relevant info, though it doesn't explain what happens when lb fails.","title":"Other Resources"},{"location":"Sort/rate-limiting-throttling/","text":"Rate Limiting / Throttling Specifically: denial of service attacks, overloading the system, attackers attempting to crack passwords You can throttle (or rate-limit) by username, IP address, region, or even across the whole system, e.g. 10 RPS globally. You can store rate-limiting info, e.g. the number of times a particular user has accessed a feature, in an in-memory database like Redis. Rate-limiting can be done in a complex way: allow a user to access a service 0.5 s between requests but only 3x every 10 and only 10x in a single minute. The response code for \"too many requests\" is 429. Rate limiting is one aspect of throttling. The other is throughput limiting. For instance, your internet bandwidth FUP plans. Or your netflix subscription plans offering different video resolutions for different subscription fees Throttling pattern - Azure Architecture Center - recommend this to complete newbie - nicely written and easy to understand (make sure to also check other links at end of this blog) How Heroku added throttling https://github.com/zombocom/rate_throttle_client - Very good implementation if you want to dig deep Design and integration of rate limiter with gateway - a very popular server side throttling pattern - gateways take the hit from user and do all throttling stuff so systems doing actual stuff don't have to worry about it Throttling from scalability perspective Sample implementation of Java client side throttling - this is popular where SDKs are provided to clients so that several checks like rate limiting can be done at client side what-is-api-throttling-and-rate-limiting - Another nice blog explaining how rate limiting can be handled at API header level https://stripe.com/blog/rate-limiters Cloudflare - How we built rate limiting capable of scaling to millions of domains AWS re:Invent 2018: Amazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database (DAT321) Figma - An alternative approach to rate limiting","title":"Rate Limiting / Throttling"},{"location":"Sort/rate-limiting-throttling/#rate-limiting-throttling","text":"Specifically: denial of service attacks, overloading the system, attackers attempting to crack passwords You can throttle (or rate-limit) by username, IP address, region, or even across the whole system, e.g. 10 RPS globally. You can store rate-limiting info, e.g. the number of times a particular user has accessed a feature, in an in-memory database like Redis. Rate-limiting can be done in a complex way: allow a user to access a service 0.5 s between requests but only 3x every 10 and only 10x in a single minute. The response code for \"too many requests\" is 429. Rate limiting is one aspect of throttling. The other is throughput limiting. For instance, your internet bandwidth FUP plans. Or your netflix subscription plans offering different video resolutions for different subscription fees Throttling pattern - Azure Architecture Center - recommend this to complete newbie - nicely written and easy to understand (make sure to also check other links at end of this blog) How Heroku added throttling https://github.com/zombocom/rate_throttle_client - Very good implementation if you want to dig deep Design and integration of rate limiter with gateway - a very popular server side throttling pattern - gateways take the hit from user and do all throttling stuff so systems doing actual stuff don't have to worry about it Throttling from scalability perspective Sample implementation of Java client side throttling - this is popular where SDKs are provided to clients so that several checks like rate limiting can be done at client side what-is-api-throttling-and-rate-limiting - Another nice blog explaining how rate limiting can be handled at API header level https://stripe.com/blog/rate-limiters Cloudflare - How we built rate limiting capable of scaling to millions of domains AWS re:Invent 2018: Amazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database (DAT321) Figma - An alternative approach to rate limiting","title":"Rate Limiting / Throttling"},{"location":"Sort/shard-rdbms/","text":"Shard RDBMS Can we shard / replicate / partition rdbms databases like nosql databases?. In my search, I see that partitioning replication are possible for rdbms. I am not sure about sharding though. Also the single most important thing is that nodqls have no need for joins, which reduce retrieval time. And this is faster than having index on rdbms database tables. There are multiple challenges, as there are multiple tables its a challenge to know how to partition them so that there is complete isolation between shards. Join operations can't be done or can be with some complex operation. Ton more, in a nutshell youll be using it at a nosql db so its not done generally. Acid properties wont be there too. Also nosqls are mostly used where joining is not needed, in cases where there is complex user data, booking data there nosql isnt needed. It is difficult to join across shards. You shard tables such that joins are required only within one shard, hence the complexity on how to decide sharding criteria Actually this is a really really big topic of discussion, mostly its about CAP theorem, acid, base properties of sql and nosql dbs, how different dbs work like cassandra. There are many YouTubers like gaurav sen who cover these well. RDBMS can be sharded : horizontal or vertical. Horizontal is when you split at row level across different nodes and vertical is when you move logically related tables to different nodes i.e. dB is split across nodes. Replication can be achieved by Master-Slave configuration, etc. Example: Google Spaner , CockroachDB . RDBMS were not built like nosql with sharding or replication out of the box. With proper planning, RDBMS can achieve those things, but it requires work.","title":"Shard RDBMS"},{"location":"Sort/shard-rdbms/#shard-rdbms","text":"Can we shard / replicate / partition rdbms databases like nosql databases?. In my search, I see that partitioning replication are possible for rdbms. I am not sure about sharding though. Also the single most important thing is that nodqls have no need for joins, which reduce retrieval time. And this is faster than having index on rdbms database tables. There are multiple challenges, as there are multiple tables its a challenge to know how to partition them so that there is complete isolation between shards. Join operations can't be done or can be with some complex operation. Ton more, in a nutshell youll be using it at a nosql db so its not done generally. Acid properties wont be there too. Also nosqls are mostly used where joining is not needed, in cases where there is complex user data, booking data there nosql isnt needed. It is difficult to join across shards. You shard tables such that joins are required only within one shard, hence the complexity on how to decide sharding criteria Actually this is a really really big topic of discussion, mostly its about CAP theorem, acid, base properties of sql and nosql dbs, how different dbs work like cassandra. There are many YouTubers like gaurav sen who cover these well. RDBMS can be sharded : horizontal or vertical. Horizontal is when you split at row level across different nodes and vertical is when you move logically related tables to different nodes i.e. dB is split across nodes. Replication can be achieved by Master-Slave configuration, etc. Example: Google Spaner , CockroachDB . RDBMS were not built like nosql with sharding or replication out of the box. With proper planning, RDBMS can achieve those things, but it requires work.","title":"Shard RDBMS"},{"location":"Sort/system-design-template/","text":"System Design Template API for client, Synchronous vs async, concurrent or chunks, push or pull, back pressure Queues \u2013 if client offline or for async operation or pub-sub (1:N, N:1, N:M) App server - ? DB/Storage \u2013 Schema, Consistency, RDMBS vs Wide col vs KeyVal vs ObjStore vs HDFS, write heavy Scaling a. Load balancer w/secondary and sticky sessions b. Sharding c. Caching for CDN or DB or App along with LRU, consistent hashing and sync with primary DB d. multiple app servers e. Deprioritize some msgs during major known events like new year Fault tolerance \u2013 Replication, monitoring, hot vs cold standby clients - encryption, heartbeats The challenge is knowing what areas to focus on in 40 minutes. Once you have a decent baseline and backing, you can end up in a too detailed design or too detailed world. Don't over do this part of preparation. Think about Can you take a vague goal (\u201ddesign Twitter\u201d) and come up with a fully-developed proposal? Are you able to spot ambiguities in the requirements and ask good clarifying questions? Are you able to distinguish between features that really need to go into the MVP versus ones that are really extended/optional features that can be punted? Do you proactively look for issues with your design, or do you need prodding from the interviewer? Are you able to assess different options and make trade-offs, or are you attached to certain ways of doing things? Do you have a good sense of what to prioritize, or do you get lost in the weeds? Are you able to produce an actual deliverable within the timeframe you are given? Does your design meet all functional and non-functional requirements? In particular, does it scale? While you are not expected to produce an industry-grade design in 45 minutes \u30fc this would be quite unreasonable to ask \u30fc what you produce should be something that can be turned over to a product team for implementation. Are you able to achieve the basic requirements quickly so that you have time to extend your design in an interesting way? Are you able to use speech, notes, and diagrams to communicate your ideas clearly to someone else? Are you able to take feedback? Example writeup from someone The MVP The first order of business in a systems design interview is to achieve the functional requirements. This typically entails designing the API and the data model. Many interviewees insist on starting out with an MVP, but in the interest of speed, I would not literally build an MVP because, as a senior engineer, there is minimal signal to be communicated to the interviewer by doing so. If you spend too much time designing an MVP, you may not have enough time to completely scale out your design and inadvertently communicate that you\u2019re more of an L4 as opposed to an L5/L6. For example, if it is clear you will need a WebSockets endpoint, skip past the GET and POST endpoints and go straight to the WebSockets endpoint. If desired, you can use the time you saved to sketch out event dispatch code for your WebSockets endpoint, which will show that you really do know what you\u2019re doing. And if you think you will eventually use NoSQL in your scaled-out design, feel free to skip past SQL and go straight there. The case for starting with NoSQL from the beginning is actually stronger than ever. For example, not only does DynamoDB support tabular data, but as of 2018, it supports ACID, including strong consistency and serializable transactions. As you start to scale your system, many of the advantages of starting with SQL become moot because you end up having to denormalize and shard your data anyway. Perhaps more importantly, data model design for SQL and NoSQL are different, so you actually do have to make this choice before designing your data model. For SQL, data model design is mostly about entities, constraints, and relationships. For NoSQL, it is about denormalization, partition keys, range keys, and secondary indices. In my experience, candidates rarely attempt a NoSQL data model, so not only would you be giving yourself a head start in scaling out your system by using NoSQL from the beginning, but you would also stand out from the other candidates. After you\u2019re done, make sure to go through all functional requirements and explain how they are achieved by your API and data model. This helps you find things you may have missed and helps the interviewer reach the conclusion that you are truly done with this phase. Scaling Your Design Next is scaling your design, that is, enhancing it so that it provides a good experience for millions of simultaneous users. Working with a diagram as you iterate on your design slows you down significantly. If it is asked of you, push off drawing the diagram until after you have fully designed the system. Only draw along the way if the interviewer is having trouble following along with you, and even then, only sketch the bare minimum needed to convey your ideas. What you should attempt to deliver instead of a diagram is a bulleted list of notes for each component. You can further increase your speed by offloading most of the work to speech. As you\u2019re thinking through the different possibilities for each decision point, simply articulate them verbally as opposed to writing them down. Make sure to check in with the interviewer after each point to ensure they are truly following along. Only jot things down once you have finalized a particular aspect of your design. By using a combination of speech, notes, and diagrams, you can complete a scaled-out design in only 20 minutes, leaving you plenty of time remaining for you to showcase advanced systems design capabilities. After you\u2019re done with your design, go through all the non-functional requirements and explain how your design delivers on them. In addition to helping you to identify when you have missed something, this final step leaves no doubt in your interviewer\u2019s mind that you have achieved all requirements before you move on to extending your design and shooting for that higher level. Above and Beyond Unlike other candidates, at this point, you\u2019ve got a solid 20 minutes left to... just play. Starting logging things to a distributed file system. Push that data through a MapReduce pipeline. Throw in a peer-to-peer protocol (\u201dLet\u2019s say things have really gone south (haha) with North Korea, and they\u2019ve used their ICBMs to take out all AWS data centers....\u201d). Maybe you can use a blockchain or zero-knowledge protocol somehow. This is your time to really show off, and yes, be a little bit silly. Rubric How to Succeed in a System Design Interview Other References Go through system design primer -> Grokking -> YT Donne Martin - System Design Primer Low Level Design primer Patterns of Distributed Systems - Martin Fowler Notion - SDI notes, same as here https://divyanshu-vibhu.gitbook.io/system-design/ https://www.linkedin.com/posts/aditya-malshikhare_system-design-basics-handbook-ugcPost-6801799624563814400-jtT6 System Design Master doc The Bible of Distributed System Design Interviews https://timilearning.com/ Reddit System Design Framework https://www.lewis-lin.com/blog/pedals-method Example - Design campaign to collect donations Distributed Scheduler SRE book LeetCode YT System Design Interview channel YT Gaurav Sen series YT Software Architecture Monday playlist YT David Malans cs75 scalability YT david huffman's talk, scaling up talk YT Why the Internet went dark for two hours - Let's discuss the Akamai outage YT - Scaling Push Messaging for Millions of Devices @Netflix Scalability for dummies Designing data intensive appliations https://blog.interviewcamp.io/live-capacity-estimation-caching-levels/ How Facebook Live Streams to 800,000 Simultaneous Viewers - High Scalability Facebook system design interview: 4 must watched videos YT Scaling Instagram Infrastructure YT Scaling Live videos to billions of users YT Live commenting at facebook YT TAO: Facebook distributed data store for social graph YT Channels YT Defog Tech YT Udit Agarwal GitHub GitHub System Architect Cheatsheet GitHub Awesome CTO GitHub Interview prep GitHub Designs GitHub Design KV Store course GitHub Mobile System design How to Use Consistent Hashing in a System Design Interview? - In a distributed system, any server responsible for a huge partition of data can become a bottleneck for the system. To handle these issues, Consistent Hashing introduces a new scheme of distributing the tokens to physical nodes. Instead of assigning a single token to a node, the hash range is divided into multiple smaller ranges, and each physical node is assigned several of these smaller ranges. Each of these subranges is considered a Vnode. Vnodes are randomly distributed across the cluster and are generally non-contiguous so that no two neighboring Vnodes are assigned to the same physical node or rack. How we rebuilt the Walmart Autocomplete Backend https://sre.google/workbook/non-abstract-design/ Google SRE Distributed ImageServer presentation Latency Numbers every programmer should know Stripe Idempotent Requests Understanding RPO and RTO | Druva How is data read? - How Cassandra uses the Bloom filter? Doordash Building Faster Indexing with Apache Kafka and Elasticsearch Building Mobile-First Infrastructure for Messenger How machine learning powers Facebook\u2019s News Feed ranking algorithm Azure Network Latency KV Memory Design LevelDB Benchmarks - The KV memory store we designed is very similar to a basic version of levelsDB - which using 2011 hardware is able to handle 700k writes per second. RocksDB Performance Benchmarks System Designs YT - Netflix System Design | YouTube System Design YT - Zoom System Design | WhatsApp / FB Video Calling System Design YT - Notification Service System Design Interview Question to handle Billions of users & Notifications","title":"System Design Template"},{"location":"Sort/system-design-template/#system-design-template","text":"API for client, Synchronous vs async, concurrent or chunks, push or pull, back pressure Queues \u2013 if client offline or for async operation or pub-sub (1:N, N:1, N:M) App server - ? DB/Storage \u2013 Schema, Consistency, RDMBS vs Wide col vs KeyVal vs ObjStore vs HDFS, write heavy Scaling a. Load balancer w/secondary and sticky sessions b. Sharding c. Caching for CDN or DB or App along with LRU, consistent hashing and sync with primary DB d. multiple app servers e. Deprioritize some msgs during major known events like new year Fault tolerance \u2013 Replication, monitoring, hot vs cold standby clients - encryption, heartbeats The challenge is knowing what areas to focus on in 40 minutes. Once you have a decent baseline and backing, you can end up in a too detailed design or too detailed world. Don't over do this part of preparation.","title":"System Design Template"},{"location":"Sort/system-design-template/#think-about","text":"Can you take a vague goal (\u201ddesign Twitter\u201d) and come up with a fully-developed proposal? Are you able to spot ambiguities in the requirements and ask good clarifying questions? Are you able to distinguish between features that really need to go into the MVP versus ones that are really extended/optional features that can be punted? Do you proactively look for issues with your design, or do you need prodding from the interviewer? Are you able to assess different options and make trade-offs, or are you attached to certain ways of doing things? Do you have a good sense of what to prioritize, or do you get lost in the weeds? Are you able to produce an actual deliverable within the timeframe you are given? Does your design meet all functional and non-functional requirements? In particular, does it scale? While you are not expected to produce an industry-grade design in 45 minutes \u30fc this would be quite unreasonable to ask \u30fc what you produce should be something that can be turned over to a product team for implementation. Are you able to achieve the basic requirements quickly so that you have time to extend your design in an interesting way? Are you able to use speech, notes, and diagrams to communicate your ideas clearly to someone else? Are you able to take feedback?","title":"Think about"},{"location":"Sort/system-design-template/#example-writeup-from-someone","text":"The MVP The first order of business in a systems design interview is to achieve the functional requirements. This typically entails designing the API and the data model. Many interviewees insist on starting out with an MVP, but in the interest of speed, I would not literally build an MVP because, as a senior engineer, there is minimal signal to be communicated to the interviewer by doing so. If you spend too much time designing an MVP, you may not have enough time to completely scale out your design and inadvertently communicate that you\u2019re more of an L4 as opposed to an L5/L6. For example, if it is clear you will need a WebSockets endpoint, skip past the GET and POST endpoints and go straight to the WebSockets endpoint. If desired, you can use the time you saved to sketch out event dispatch code for your WebSockets endpoint, which will show that you really do know what you\u2019re doing. And if you think you will eventually use NoSQL in your scaled-out design, feel free to skip past SQL and go straight there. The case for starting with NoSQL from the beginning is actually stronger than ever. For example, not only does DynamoDB support tabular data, but as of 2018, it supports ACID, including strong consistency and serializable transactions. As you start to scale your system, many of the advantages of starting with SQL become moot because you end up having to denormalize and shard your data anyway. Perhaps more importantly, data model design for SQL and NoSQL are different, so you actually do have to make this choice before designing your data model. For SQL, data model design is mostly about entities, constraints, and relationships. For NoSQL, it is about denormalization, partition keys, range keys, and secondary indices. In my experience, candidates rarely attempt a NoSQL data model, so not only would you be giving yourself a head start in scaling out your system by using NoSQL from the beginning, but you would also stand out from the other candidates. After you\u2019re done, make sure to go through all functional requirements and explain how they are achieved by your API and data model. This helps you find things you may have missed and helps the interviewer reach the conclusion that you are truly done with this phase. Scaling Your Design Next is scaling your design, that is, enhancing it so that it provides a good experience for millions of simultaneous users. Working with a diagram as you iterate on your design slows you down significantly. If it is asked of you, push off drawing the diagram until after you have fully designed the system. Only draw along the way if the interviewer is having trouble following along with you, and even then, only sketch the bare minimum needed to convey your ideas. What you should attempt to deliver instead of a diagram is a bulleted list of notes for each component. You can further increase your speed by offloading most of the work to speech. As you\u2019re thinking through the different possibilities for each decision point, simply articulate them verbally as opposed to writing them down. Make sure to check in with the interviewer after each point to ensure they are truly following along. Only jot things down once you have finalized a particular aspect of your design. By using a combination of speech, notes, and diagrams, you can complete a scaled-out design in only 20 minutes, leaving you plenty of time remaining for you to showcase advanced systems design capabilities. After you\u2019re done with your design, go through all the non-functional requirements and explain how your design delivers on them. In addition to helping you to identify when you have missed something, this final step leaves no doubt in your interviewer\u2019s mind that you have achieved all requirements before you move on to extending your design and shooting for that higher level. Above and Beyond Unlike other candidates, at this point, you\u2019ve got a solid 20 minutes left to... just play. Starting logging things to a distributed file system. Push that data through a MapReduce pipeline. Throw in a peer-to-peer protocol (\u201dLet\u2019s say things have really gone south (haha) with North Korea, and they\u2019ve used their ICBMs to take out all AWS data centers....\u201d). Maybe you can use a blockchain or zero-knowledge protocol somehow. This is your time to really show off, and yes, be a little bit silly.","title":"Example writeup from someone"},{"location":"Sort/system-design-template/#rubric","text":"How to Succeed in a System Design Interview","title":"Rubric"},{"location":"Sort/system-design-template/#other-references","text":"Go through system design primer -> Grokking -> YT Donne Martin - System Design Primer Low Level Design primer Patterns of Distributed Systems - Martin Fowler Notion - SDI notes, same as here https://divyanshu-vibhu.gitbook.io/system-design/ https://www.linkedin.com/posts/aditya-malshikhare_system-design-basics-handbook-ugcPost-6801799624563814400-jtT6 System Design Master doc The Bible of Distributed System Design Interviews https://timilearning.com/ Reddit System Design Framework https://www.lewis-lin.com/blog/pedals-method Example - Design campaign to collect donations Distributed Scheduler SRE book LeetCode YT System Design Interview channel YT Gaurav Sen series YT Software Architecture Monday playlist YT David Malans cs75 scalability YT david huffman's talk, scaling up talk YT Why the Internet went dark for two hours - Let's discuss the Akamai outage YT - Scaling Push Messaging for Millions of Devices @Netflix Scalability for dummies Designing data intensive appliations https://blog.interviewcamp.io/live-capacity-estimation-caching-levels/ How Facebook Live Streams to 800,000 Simultaneous Viewers - High Scalability Facebook system design interview: 4 must watched videos YT Scaling Instagram Infrastructure YT Scaling Live videos to billions of users YT Live commenting at facebook YT TAO: Facebook distributed data store for social graph YT Channels YT Defog Tech YT Udit Agarwal GitHub GitHub System Architect Cheatsheet GitHub Awesome CTO GitHub Interview prep GitHub Designs GitHub Design KV Store course GitHub Mobile System design How to Use Consistent Hashing in a System Design Interview? - In a distributed system, any server responsible for a huge partition of data can become a bottleneck for the system. To handle these issues, Consistent Hashing introduces a new scheme of distributing the tokens to physical nodes. Instead of assigning a single token to a node, the hash range is divided into multiple smaller ranges, and each physical node is assigned several of these smaller ranges. Each of these subranges is considered a Vnode. Vnodes are randomly distributed across the cluster and are generally non-contiguous so that no two neighboring Vnodes are assigned to the same physical node or rack. How we rebuilt the Walmart Autocomplete Backend https://sre.google/workbook/non-abstract-design/ Google SRE Distributed ImageServer presentation Latency Numbers every programmer should know Stripe Idempotent Requests Understanding RPO and RTO | Druva How is data read? - How Cassandra uses the Bloom filter? Doordash Building Faster Indexing with Apache Kafka and Elasticsearch Building Mobile-First Infrastructure for Messenger How machine learning powers Facebook\u2019s News Feed ranking algorithm Azure Network Latency KV Memory Design LevelDB Benchmarks - The KV memory store we designed is very similar to a basic version of levelsDB - which using 2011 hardware is able to handle 700k writes per second. RocksDB Performance Benchmarks System Designs YT - Netflix System Design | YouTube System Design YT - Zoom System Design | WhatsApp / FB Video Calling System Design YT - Notification Service System Design Interview Question to handle Billions of users & Notifications","title":"Other References"},{"location":"Sort/transactions-in-shopping/","text":"Transactions in Shopping How would you solve a problem where you have a highly distributed system like Amazon and you need to implement shopping cart and ensure that ( ie if one item left in inventory) two or more users can't add that item to the cart? Let's say first user adds the item and gets a ten minute lock on it or else it's relinquished. At first I'm thinking sql for atomic transactions and not allowing inventory count to go zero, but I feel the sql database may be a bottle neck at Amazon scale. Would nosql be the way to go? If so, how do you guarantee strong consistency of adding to cart in relation to inventory? And I feel sql db will not scale because single write db would be a bottleneck. Even if sharding by product ID or something like that, then you have to consider regions globally too. Inventory count varies from regions, locations. I think we should still allow users to place order, and the order processing system should fulfill order based on order placement time. And later the other user can be communicated about the lack of inventory and initiate the refund. It happened to me few times in amazon shopping. This depends on scale and it is very subjective. Discuss both options with the interviewer. No SQL is a better approach. And rater than blocking allow users to add it to their cart and checkout. During payment, the first payment that succeeds gets the item, for rest we can issue a refund within next 2 minutes. At the payment page it acquires the lock for X mins and if payment fails or user haven't placed then the Thread lock will be released. If so we are dealing with at most once semantics in near real time and exactly once after some drift, it becomes a slightly simpler problem. How do you implement the locking on inventory in distributed nosql? You should ask for read modify write type of support from systems. I.e don't take the data in memory ( Google photon paper mentions it very briefly) and modify it and then write again. That's suboptimal strategy. If your db support read modify write pattern the computation happens in node itself. The challenge you'll run into if you go for non quorum based consistency. So each node has a different count now under network partitioning. Well also need cleanup strategy for this which can happen at a later time. So the way it works a) increment-state-if(0) b) after puchase complete increment-state-if(1). System has a background job to cleanup if client crashed. You can optimize it by asking client to register some call-back for health checks. How do you implement the locking on inventory in distributed nosql? We won't need db transactions if we use optimistic locks. Optimistic locking is just versioning. Not like we have an exclusive lock. On write, you check the version number you got on the read still the same. So you can have many people read (as is our use case) but only one guy will succeed in writing. So, no extra headers of traditional locking. YT Transactions Internal implementation write ahead log and locks with banking examples","title":"Transactions in Shopping"},{"location":"Sort/transactions-in-shopping/#transactions-in-shopping","text":"How would you solve a problem where you have a highly distributed system like Amazon and you need to implement shopping cart and ensure that ( ie if one item left in inventory) two or more users can't add that item to the cart? Let's say first user adds the item and gets a ten minute lock on it or else it's relinquished. At first I'm thinking sql for atomic transactions and not allowing inventory count to go zero, but I feel the sql database may be a bottle neck at Amazon scale. Would nosql be the way to go? If so, how do you guarantee strong consistency of adding to cart in relation to inventory? And I feel sql db will not scale because single write db would be a bottleneck. Even if sharding by product ID or something like that, then you have to consider regions globally too. Inventory count varies from regions, locations. I think we should still allow users to place order, and the order processing system should fulfill order based on order placement time. And later the other user can be communicated about the lack of inventory and initiate the refund. It happened to me few times in amazon shopping. This depends on scale and it is very subjective. Discuss both options with the interviewer. No SQL is a better approach. And rater than blocking allow users to add it to their cart and checkout. During payment, the first payment that succeeds gets the item, for rest we can issue a refund within next 2 minutes. At the payment page it acquires the lock for X mins and if payment fails or user haven't placed then the Thread lock will be released. If so we are dealing with at most once semantics in near real time and exactly once after some drift, it becomes a slightly simpler problem. How do you implement the locking on inventory in distributed nosql? You should ask for read modify write type of support from systems. I.e don't take the data in memory ( Google photon paper mentions it very briefly) and modify it and then write again. That's suboptimal strategy. If your db support read modify write pattern the computation happens in node itself. The challenge you'll run into if you go for non quorum based consistency. So each node has a different count now under network partitioning. Well also need cleanup strategy for this which can happen at a later time. So the way it works a) increment-state-if(0) b) after puchase complete increment-state-if(1). System has a background job to cleanup if client crashed. You can optimize it by asking client to register some call-back for health checks. How do you implement the locking on inventory in distributed nosql? We won't need db transactions if we use optimistic locks. Optimistic locking is just versioning. Not like we have an exclusive lock. On write, you check the version number you got on the read still the same. So you can have many people read (as is our use case) but only one guy will succeed in writing. So, no extra headers of traditional locking. YT Transactions Internal implementation write ahead log and locks with banking examples","title":"Transactions in Shopping"}]}